{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d7bb61",
   "metadata": {},
   "source": [
    "# CS336 Assignment 4 (data): Filtering Language Modeling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1a34e",
   "metadata": {},
   "source": [
    "## 2 Filtering Common Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf242919",
   "metadata": {},
   "source": [
    "### Problem (look_at_cc): 4 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61e2c6",
   "metadata": {},
   "source": [
    "Here is the modified text based on your request. The `<span>` separator has been changed to a `<div>` with the new style.\n",
    "\n",
    "(a) Download the WARC file above, or find the copy we provide on the cluster. Let’s look at the first page in this file. This is a gzipped file, and you can browse its contents with:\n",
    "`$ zcat /data/CC/example.warc.gz | less`\n",
    "`less` lets you browse the file using keyboard arrows, Page Up, Page Down. To exit, press “q”. Look at the very first web page. What is its URL? Is it still accessible? Can you tell what the page seems to be about by looking at the raw HTML?\n",
    "\n",
    "**Deliverable**: A 2-3 sentence response.\n",
    "\n",
    "-----\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "**URL**: http://0371rykj.com/ipfhsb/34.html\n",
    "\n",
    "it seems to no longer be active.\n",
    "\n",
    "the page seems to be about a company called \"上海林頻儀器股份有限公司\", which appears to manufacture and sell environmental testing equipment, such as \"恒溫恒濕試驗箱\". The page lists various product models and their specifications.\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "(b) Let’s now look at the corresponding WET file:\n",
    "`$ zcat /data/CC/example.warc.wet.gz | less`\n",
    "Note that the WET files contain HTTP headers (e.g., Content-Length) that are not part of the extracted text contents. If you look at the first example, you will see that it contains text that was extracted from the raw HTML you just saw.\n",
    "Notice that much of the extracted text is reminiscent of the HTML structure, and not actually the page’s main content. Are there parts of the text you see that you think should have been filtered out by the extractor? Think about the quality of this text as training data: what might go wrong in training a model on text that looks like this? Conversely, what useful information can a model potentially extract from this page?\n",
    "\n",
    "**Deliverable**: A 3-4 sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "The repetitive navigation links and boilerplate text like contact info should be filtered out, as they aren’t core content—for instance, \"服務熱線：4000 662 888\" appears multiple times.\n",
    "\n",
    "Training on such text could cause a model to overemphasize repeated or structural elements, leading to irrelevant or repetitive outputs.\n",
    "\n",
    "However, a model could still extract useful information. It can learn about the products offered (e.g., \"恒溫恒濕試驗(yàn)箱\"), the company's name (\"上海林頻儀器股份有限公司\"), and specific details like product models and dimensions from the tables, which could be valuable for tasks like product categorization or information retrieval.\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "(c) What makes a good training example is highly contextual. Describe an application domain for which this example might be useful to have in the training data, and one where it might not be.\n",
    "\n",
    "**Deliverable**: A 1-2 sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "**might be useful**: training QA or extraction models in industrial supply chains; it teaches the model to pull structured product and contact info from varied web formats.\n",
    "\n",
    "**might not be**: training general language or summarization models, as cluttered layout and repeated terms may harm fluency and lead to unnatural outputs.\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "(d) Let’s look at some more examples to get a better sense of what’s in the Common Crawl. Look through 25 more WET records. For each record, very briefly comment on the document’s language (if you can identify it), the domain name, what type of page it is, etc. How many examples does it take until you see what you’d deem a “high-quality” webpage?\n",
    "\n",
    "**Deliverable**: Brief annotations of 25 documents with the document’s language, domain, type of page, and any other miscellaneous notes about the document. The number of examples it takes until you see a high-quality example.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "Only 1 out of 25 is of high quality, luckily it appeared in second place.\n",
    "\n",
    "| # | Domain | Language | Page Type | Quality Assessment | Notes |\n",
    "|---|--------|----------|-----------|-------------------|-------|\n",
    "| 1 | 10www.chinatikfans.com | Chinese/English (zho,eng) | Forum/Blog | Low | Thai actor fan forum, outdated content from 2010, poor structure |\n",
    "| 2 | 13.usnccm.org | English (eng) | Conference website | **High** | Academic conference website, well-structured, professional content |\n",
    "| 3 | 176.utchat888.com | Chinese (zho) | Adult chat | Very Low | Adult video chat site, explicit content, poor quality |\n",
    "| 4 | 176766.cn | Chinese/Japanese (jpn,zho) | Product catalog | Low | Chinese industrial equipment, minimal content, poor formatting |\n",
    "| 5 | 178mh.com | Chinese (zho) | Error page | Very Low | Template error, no meaningful content |\n",
    "| 6 | 1796370.tgtg97.com | Chinese (zho) | Adult chat | Very Low | Adult video chat, explicit content, poor quality |\n",
    "| 7 | 18sex.v340.info | Chinese/English (zho,eng) | Adult content | Very Low | Adult content site, explicit material |\n",
    "| 8 | 1kb.klimtoren.be | Dutch/English (nld,eng) | Personal blog | Low | Dutch teacher's blog, simple content, outdated |\n",
    "| 9 | 1pekesat-exae.mysch.gr | Greek/English (ell,eng) | Forum | Low | Greek educational forum, mixed with spam content |\n",
    "| 10 | 1pekesat-exae.mysch.gr | Greek/English (ell,eng) | Login page | Low | Forum login page, minimal content |\n",
    "| 11 | 1s6605084.yhxzseo.com | Chinese (zho) | E-commerce | Low | Chinese e-commerce site, product listings, poor formatting |\n",
    "| 12 | 20com20.fr | Turkish/Danish/English (tur,dan,eng) | Documentation | Medium | Apache server documentation, technical but useful |\n",
    "| 13 | 24ktcasino.net | English (eng) | Blog/Article | Low | Casino blog, affiliate content, low quality |\n",
    "| 14 | 2kgames.eu | English (eng) | Error page | Very Low | 404 error, no content |\n",
    "| 15 | 2l6185919.yizhangting.com | Chinese (zho) | News/Health | Low | Chinese health news site, minimal content |\n",
    "| 16 | 303323.com | Chinese (zho) | Contact page | Very Low | Just contact information, no meaningful content |\n",
    "| 17 | 30bad.com | Chinese (zho) | Entertainment | Low | Chinese entertainment site, poor quality content |\n",
    "| 18 | 312001.net | Chinese (zho) | Empty page | Very Low | Nearly empty page, no content |\n",
    "| 19 | 354577.mwe075.com | Chinese (zho) | Adult chat | Very Low | Adult video chat, explicit content |\n",
    "| 20 | 356.schoollibrary.edu.pe.ca | English (eng) | Library catalog | Medium | Library search results, no results found |\n",
    "| 21 | 366392.haaxz.com | Chinese (zho) | Adult chat | Very Low | Adult video chat, explicit content |\n",
    "| 22 | 356.schoollibrary.edu.pe.ca | English (eng) | Library catalog | Medium | Library search results, no results found |\n",
    "| 23 | 366392.haaxz.com | Chinese (zho) | Adult chat | Very Low | Adult video chat, explicit content |\n",
    "| 24 | 366392.haaxz.com | Chinese (zho) | Adult chat | Very Low | Adult video chat, explicit content |\n",
    "| 25 | 387tel.com | Chinese/English (zho,eng) | Adult chat | Very Low | Adult video chat site, explicit content |\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6232",
   "metadata": {},
   "source": [
    "### Problem (extract_text): 3 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad072e",
   "metadata": {},
   "source": [
    "(a) Write a function that extracts text from a byte string containing raw HTML. Use `resiliparse.extract.html2text.extract_plain_text` to perform the extraction. This function needs a string, so you will need to first decode the byte string into a Unicode string. Be aware that the input byte string might not be encoded in UTF-8, so your function should be able to detect the encoding in case UTF-8 fails. Resiliparse also offers `resiliparse.parse.encoding.detect_encoding()`, which might be useful.\n",
    "\n",
    "**Deliverable**: A function that takes a byte string containing HTML and returns a string containing the extracted text. Implement the adapter `[run_extract_text_from_html_bytes]` and make sure it passes `uv run pytest -k test_extract_text_from_html_bytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83972cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment4-data\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2, hydra-core-1.3.2\n",
      "collected 21 items / 20 deselected / 1 selected\n",
      "\n",
      "tests/test_extract.py::test_extract_text_from_html_bytes \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m20 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_extract_text_from_html_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b9166",
   "metadata": {},
   "source": [
    "(b) Run your text extraction function on a single WARC file. Compare its output to the extracted text in the corresponding WET file. What differences and/or similarities do you notice? Which extraction seems better?\n",
    "\n",
    "**Deliverable**: 2-3 sentence response comparing and contrasting the text extracted by your own function versus the extracted text in the WET files.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "**WET extraction** produces cleaner, more readable text by **stripping HTML** and reducing noise, making it better for NLP tasks. \n",
    "\n",
    "In contrast, **my extraction function** retains more original structure and formatting, which may be useful when preserving layout or HTML elements is important. \n",
    "\n",
    "WET seems better in general.\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5bb865",
   "metadata": {},
   "source": [
    "### Problem (language_identification): 6 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef72a22",
   "metadata": {},
   "source": [
    "(a) Write a function that will take a Unicode string and identify the main language that is present in this string. Your function should return a pair, containing an identifier of the language and a score between 0 and 1 representing its confidence in that prediction.\n",
    "\n",
    "**Deliverable**: A function that performs language identification, giving its top language prediction and a score. Implement the adapter `[run_identify_language]` and make sure it passes both tests in `uv run pytest -k test_identify_language`. Note that these tests assume a particular string identifier for English (“en”) and Chinese (“zh”), so your test adapter should perform any applicable re-mapping, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49635a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment4-data\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2, hydra-core-1.3.2\n",
      "collected 21 items / 19 deselected / 2 selected\n",
      "\n",
      "tests/test_langid.py::test_identify_language_english \u001b[32mPASSED\u001b[0m\n",
      "tests/test_langid.py::test_identify_language_chinese_simplified \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m19 deselected\u001b[0m\u001b[32m in 0.51s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_identify_language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a595fd",
   "metadata": {},
   "source": [
    "(b) The behavior of language models at inference time largely depends on the data they were trained on. As a result, issues in the data filtering pipeline can result in problems downstream. What issues do you think could arise from problems in the language identification procedure? In a higher-stakes scenario (such as when deploying a user-facing product), how would you go about mitigating these issues?\n",
    "\n",
    "**Deliverable**: A 2-5 sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "Errors in language identification can pollute training data with unintended languages, leading to mixed-language outputs, degraded fluency, or biased performance. \n",
    "\n",
    "To mitigate this, a multi-layered strategy should be used: ensemble classifiers with high confidence thresholds, manual review of samples, and ongoing monitoring for quality issues and bias.\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "(c) Run your language identification system on text extracted from the WARC files (via your previously-implemented text extraction function). Manually identify the language in 20 random examples and compare your labels with the classifier predictions. Report any classifier errors. What fraction of documents are English? Based on your observations, what would be a suitable classifier confidence threshold to use in filtering?\n",
    "\n",
    "**Deliverable**: A 2-5 sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "Luckily, the language classifier achieved 100% accuracy on a 20-document sample, with 25% identified as English. However, some correct predictions had low confidence scores (e.g., 0.357 for Croatian, 0.593 for English). \n",
    "\n",
    "To ensure a high-purity dataset, a confidence threshold of 0.85 is recommended to filter out low-certainty predictions.\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b41f5",
   "metadata": {},
   "source": [
    "### Problem (mask_pii): 3 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e1484",
   "metadata": {},
   "source": [
    "1.  Write a function to mask out emails. Your function will take a string as input, and replace all instances of email addresses with the string \"|||EMAIL_ADDRESS|||\". To detect email addresses, you can look up regular expressions that do this reliably.\n",
    "\n",
    "**Deliverable**: A function that replaces all email addresses in a given string with the string \"|||EMAIL_ADDRESS|||\", returning a pair containing both the new string and the number of instances that were masked. Implement the adapter `[run_mask_emails]` and make sure it passes all tests in `uv run pytest -k test_mask_emails`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c94369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment4-data\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2, hydra-core-1.3.2\n",
      "collected 21 items / 18 deselected / 3 selected\n",
      "\n",
      "tests/test_pii.py::test_mask_emails_single \u001b[32mPASSED\u001b[0m\n",
      "tests/test_pii.py::test_mask_emails_multiple \u001b[32mPASSED\u001b[0m\n",
      "tests/test_pii.py::test_mask_emails_existing_string \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m3 passed\u001b[0m, \u001b[33m18 deselected\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_mask_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b1616",
   "metadata": {},
   "source": [
    "2.  Write a function to mask out phone numbers. Your function will take a string as input, and replace all instances of phone numbers with the string \"|||PHONE_NUMBER|||\". Doing this reliably can be extremely challenging, as phone numbers might be written in an extremely diverse set of formats, but you should try to capture at least the most common phone number formats used in the United States, and be robust to minor syntactic deviations.\n",
    "\n",
    "**Deliverable**: A function that replaces phone numbers in a given string with the string \"|||PHONE_NUMBER|||\", returning a pair containing both the new string and the number of instances that were masked. Implement the adapter `[run_mask_phone_numbers]` and make sure it passes `uv run pytest -k test_mask_phones`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63676780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment4-data\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2, hydra-core-1.3.2\n",
      "collected 21 items / 20 deselected / 1 selected\n",
      "\n",
      "tests/test_pii.py::test_mask_phones_single \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m20 deselected\u001b[0m\u001b[32m in 0.14s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_mask_phones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555570d3",
   "metadata": {},
   "source": [
    "3.  Write a function to mask out IP addresses. For this problem, it is enough to focus on IPv4 addresses (4 numbers up to 255 separated by points). Your function will take a string as input, and replace all instances of IP addresses with the string \"|||IP_ADDRESS|||\".\n",
    "\n",
    "**Deliverable**: A function that replaces IPv4 addresses in a given string with the string \"|||IP_ADDRESS|||\", returning a pair containing both the new string and the number of instances that were masked. Implement the adapter `[run_mask_ips]` and make sure it passes `uv run pytest -k test_mask_ips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ebd54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment4-data\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2, hydra-core-1.3.2\n",
      "collected 21 items / 20 deselected / 1 selected\n",
      "\n",
      "tests/test_pii.py::test_mask_ips \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m20 deselected\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_mask_ips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eefcfb",
   "metadata": {},
   "source": [
    "4.  What problems do you think might arise downstream in a language model when these filters are naively applied on the training set? How might you mitigate these issues?\n",
    "\n",
    "**Deliverable**: A 2-5 sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "Naive filters may corrupt training data by mislabeling elements (e.g., version numbers as IPs), confusing the model and hindering its grasp of PII structure. This can lead to unnatural outputs like `|||EMAIL_ADDRESS|||`.\n",
    "\n",
    "Using context-aware methods like NER and replacing sensitive data with realistic synthetic examples helps maintain data integrity.\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "5.  Run your PII masking functions on text extracted from the WARC files (via your previously-implemented text extraction function). Look through 20 random examples where a replacement was made; give some examples of false positives and false negatives.\n",
    "\n",
    "**Deliverable**: A 2-5 sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "Based on the provided results, the PII masking function for phone numbers has notable **false positives**, frequently misidentifying four-digit years and other numbers as telephone numbers. For example, years like \"2022\" and \"2018\" were incorrectly masked as phone numbers, as was a long numeric string in a PDF creation date.\n",
    "\n",
    "Conversely, no obvious **false negatives**, such as missed email addresses or phone numbers, were observed in the sample.\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a0b8f",
   "metadata": {},
   "source": [
    "### Problem (harmful_content): 6 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232da6a",
   "metadata": {},
   "source": [
    "1.  Write a function to detect NSFW content.\n",
    "\n",
    "**Deliverable**: A function that labels a given string as containing NSFW content or not, returning a pair containing both the label and a confidence score. Implement the adapter `[run_classify_nsfw]` and make sure it passes `uv run pytest -k test_classify_nsfw`. Note that this test is just a sanity check, taken from the Jigsaw dataset, but by no means asserts that your classifier is accurate, which you should validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1636253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment4-data\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2, hydra-core-1.3.2\n",
      "collected 21 items / 20 deselected / 1 selected\n",
      "\n",
      "tests/test_toxicity.py::test_classify_nsfw \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m20 deselected\u001b[0m\u001b[32m in 6.95s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_classify_nsfw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e997b96",
   "metadata": {},
   "source": [
    "2.  Write a function to detect toxic speech.\n",
    "\n",
    "**Deliverable**: A function that labels a given string as consisting of toxic speech or not, returning a pair containing both the label and a confidence score. Implement the adapter `[run_classify_toxic_speech]` and make sure it passes `uv run pytest -k test_classify_toxic_speech`. Again, this test is just a sanity check, also taken from Jigsaw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2e602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment4-data\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2, hydra-core-1.3.2\n",
      "collected 21 items / 20 deselected / 1 selected\n",
      "\n",
      "tests/test_toxicity.py::test_classify_toxic_speech \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m20 deselected\u001b[0m\u001b[32m in 5.45s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_classify_toxic_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a720c888",
   "metadata": {},
   "source": [
    "3.  What problems do you think might arise downstream in a language model when these filters are applied to create the training set? How might you mitigate these issues?\n",
    "\n",
    "**Deliverable**: A 2-5 sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "Overly strict content filters can introduce bias by wrongly flagging non-toxic discussions, especially from minority groups, reducing the model’s ability to handle nuanced topics. \n",
    "\n",
    "To address this, use context-aware classifiers and involve human reviewers to ensure important context is preserved.\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "4.  Run your harmful content filters on text extracted from the WARC files (via your previouslyimplemented text extraction function). Look through 20 random examples and compare the classifier predictions to your own judgments. Report any classifier errors. What fraction of documents are harmful? Based on your observations, what would be suitable classifier confidence threshold(s) to use in filtering?\n",
    "\n",
    "**Deliverable**: A 2-5 sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "A review of 20 random examples showed that classifiers for NSFW and toxic speech made no clear errors, correctly labeling all documents as non-harmful. The observed harmful content rate was 0%. \n",
    "\n",
    "A confidence threshold of 0.85 or higher is reasonable for reducing false positives.\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c232ff6",
   "metadata": {},
   "source": [
    "### Problem (gopher_quality_filters): 3 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f45d14",
   "metadata": {},
   "source": [
    "(a) Implement (at least) the subset of the Gopher quality filters as described above. For tokenizing text into words, you might find the NLTK package useful (specifically `nltk.word_tokenize`), though you’re not required to use it.\n",
    "\n",
    "**Deliverable**: A function that takes a string as its only argument and returns a boolean indicating whether the text passes the Gopher quality filters. Implement the adapter `[run_gopher_quality_filter]`. Then, make sure your filters pass the tests in `uv run pytest -k test_gopher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03df835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment4-data\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2, hydra-core-1.3.2\n",
      "collected 21 items / 14 deselected / 7 selected\n",
      "\n",
      "tests/test_quality.py::test_gopher_valid_input \u001b[32mPASSED\u001b[0m\n",
      "tests/test_quality.py::test_gopher_less_than_50_non_symbol_words \u001b[32mPASSED\u001b[0m\n",
      "tests/test_quality.py::test_gopher_more_than_100000_non_symbol_words \u001b[32mPASSED\u001b[0m\n",
      "tests/test_quality.py::test_gopher_average_word_length_less_than_3 \u001b[32mPASSED\u001b[0m\n",
      "tests/test_quality.py::test_gopher_average_word_length_greater_than_10 \u001b[32mPASSED\u001b[0m\n",
      "tests/test_quality.py::test_gopher_more_than_30_percent_lines_ending_with_ellipsis \u001b[32mPASSED\u001b[0m\n",
      "tests/test_quality.py::test_gopher_less_than_80_percent_words_with_alphabetic_character \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m7 passed\u001b[0m, \u001b[33m14 deselected\u001b[0m\u001b[32m in 3.92s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_gopher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b41c59",
   "metadata": {},
   "source": [
    "(b) Run your rule-based quality filter on text extracted from the WARC files (via your previously-implemented text extraction function). Look through 20 random examples and compare the filter predictions to your own judgment. Comment on any cases where the quality filters differ from your judgments.\n",
    "\n",
    "**Deliverable**: A 2-5 sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "My rule-based filter often disagreed with the `is_harmful` flags, rejecting more low-quality records—mainly navigational links, boilerplate, or repetitive content. \n",
    "\n",
    "The provided filter seemed **too permissive**, letting many non-substantive pages through.\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ad748",
   "metadata": {},
   "source": [
    "### Problem (quality_classifier): 15 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27773f67",
   "metadata": {},
   "source": [
    "(a) Train a quality classifier that, given text, returns a numeric quality score.\n",
    "\n",
    "**Deliverable**: A quality classifier for use in the next subproblem.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "(b) Write a function that labels a page as high or low-quality, and provides a confidence score in the label.\n",
    "\n",
    "**Deliverable**: A function taking a string as its only argument, and returning a pair with a label (high-quality or not) and a confidence score. Implement the adapter `[run_classify_quality]`. As a sanity check, make sure it correctly classifies the two examples we provide by running `uv run pytest -k test_classify_quality`.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeed1a3",
   "metadata": {},
   "source": [
    "## 3 Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb056851",
   "metadata": {},
   "source": [
    "### Problem (exact_deduplication): 3 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d2770",
   "metadata": {},
   "source": [
    "Write a function that takes a list of paths to input files and performs exact line deduplication on them. It should first count the frequency of each line in the corpus, using a hash to reduce memory, and then rewrite each file by only keeping its unique lines.\n",
    "\n",
    "**Deliverable**: A function that performs exact line deduplication. Your function should take two arguments: (a) a list of paths to its input files, and (b) an output directory. It should rewrite each input file to the output directory with the same name, but deduplicate the content by removing lines that occur more than once in the set of input files. For example, if the input paths are `a/1.txt` and `a/2.txt`, and the output directory is `b/`, your function should write the files `b/1.txt` and `b/2.txt`. Implement the adapter `[run_exact_line_deduplication]` and make sure it passes `uv run pytest -k test_exact_line_deduplication`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57677958",
   "metadata": {},
   "outputs": [],
   "source": [
    "uv run pytest -k test_exact_line_deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc998b",
   "metadata": {},
   "source": [
    "### Problem (minhash_deduplication): 8 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d20fdd0",
   "metadata": {},
   "source": [
    "Write a function that takes a list of paths to input files and performs fuzzy document deduplication with minhash and LSH. In particular, your function should compute minhash signatures for each document in the provided list of paths, use LSH with the provided number of bands to identify candidate duplicates, and then compute the true ngram Jaccard similarity between candidate duplicates and remove those that exceed a given threshold. To improve recall (following Penedo et al., 2023), normalize the text before computing minhash signatures and/or comparing Jaccard similarity by lowercasing, removing punctuation, normalizing whitespaces, and removing accents, and applying NFD unicode normalization.\n",
    "\n",
    "**Deliverable**: A function that performs fuzzy document deduplication. Your function should take at least the following arguments: (a) a list of paths to its input files, (b) the number of hashes to use for computing minhash signatures, (c) the number of bands to use for LSH, (d) the n-gram length (in words) for computing minhash signatures, and (e) an output directory. You may assume that the number of hashes to use for computing minhash signatures is evenly divisible by the number of bands to use for LSH.\n",
    "\n",
    "Your function should rewrite each input file to the output directory with the same name, but only writing documents that are either (a) not candidate duplicates and/or (b) are randomly selected to be retained from the clustered buckets. For example, if the input paths are `a/1.txt` and `a/2.txt`, and the output directory is `b/`, your function should write the files `b/1.txt` and `b/2.txt`. Implement the adapter `[run_minhash_deduplication]` and make sure it passes `uv run pytest -k test_minhash_deduplication`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad398c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment4-data\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.2, hydra-core-1.3.2\n",
      "collected 21 items / 19 deselected / 2 selected\n",
      "\n",
      "tests/test_deduplication.py::test_minhash_deduplication_exact_duplicates \u001b[32mPASSED\u001b[0m\n",
      "tests/test_deduplication.py::test_minhash_deduplication_fuzzy_duplicates \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m19 deselected\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_minhash_deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbb56a",
   "metadata": {},
   "source": [
    "## 4 Leaderboard: filter data for language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e488616",
   "metadata": {},
   "source": [
    "### Problem (filter_data): 6 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502112cc",
   "metadata": {},
   "source": [
    "(a) Write a script to filter language modeling data from a collection of Common Crawl WET files (located at `/data/CC/CC*.warc.wet.gz` on the Together cluster). You are free to apply any of the primitives we’ve implemented in earlier parts of the assignment, and you’re also free to explore other filters and methods for generating data (e.g., filtering based on n-gram language model perplexity). Your goal is to produce data that, when trained on, minimizes the perplexity on the C4 100 domains subset of the Paloma benchmark.\n",
    "\n",
    "Again, we note that you are allowed to make use of the Paloma validation data in constructing filters or classifiers to process the CC WET files, but are not allowed to literally copy any of the validation data into your training data.\n",
    "\n",
    "Your script should report the number of examples kept by each filter that you’ve used, so you have a sense of how the filters are contributing to the final output data.\n",
    "\n",
    "**Deliverable**: A script (or sequence of scripts) that filters the provided CC WET files in parallel to produce language modeling data. A written breakdown of what proportion of the discarded examples are removed by each filter step.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "(b) How long does it take to filter the 5,000 WET files? How long would it take to filter the entire Common Crawl dump (100,000 WETs)?\n",
    "\n",
    "**Deliverable**: Runtime of the data filtering pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfa14a",
   "metadata": {},
   "source": [
    "### Problem (inspect_filtered_data): 4 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d907e1",
   "metadata": {},
   "source": [
    "(a) Take five random examples from your filtered dataset. Comment on their quality and whether or not they’d be suitable for language modeling, especially given that our goal is to minimize perplexity on the C4 100 domains benchmark.\n",
    "\n",
    "**Deliverable**: Five random examples from the final filtered data. Only showing pertinent excerpts of the data is fine, since the documents may be lengthy. For each example, a 1-2 sentence description of the example and whether or not it’s worthwhile to use for language modeling.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "(b) Take five CC WETs that were removed and/or modified by your filtering script. What part of your filtering process removed or modified these documents, and do you think that their removal and/or modification was justified?\n",
    "\n",
    "**Deliverable**: Five random discarded examples from the original WETs. Only showing pertinent excerpts of the data is fine, since the documents may be lengthy. For each example, a 1-2 sentence description of the example and whether or not its removal was justified.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "(c) If your analysis above motivates further changes to your data pipeline, feel free to make those changes before training your model. Report any changes and/or iterations of data that you experimented with.\n",
    "\n",
    "**Deliverable**: A description of data changes and/or iterations that you experimented with.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448133be",
   "metadata": {},
   "source": [
    "### Problem (tokenize_data): 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ff5c8",
   "metadata": {},
   "source": [
    "Write a script to tokenize and serialize your filtered data. Make sure to serialize following the example code above, with `ids_array.tofile(output_path)`, where `ids_array` is a `np.uint16` numpy array of integer IDs. This ensures compatibility with the provided training script. How many tokens are in your filtered dataset?\n",
    "\n",
    "**Deliverable**: A script to tokenize and serialize your filtered data, and the number of tokens in your produced dataset.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5b4c2",
   "metadata": {},
   "source": [
    "### Problem (train_model): 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40cf13",
   "metadata": {},
   "source": [
    "Train a language model (GPT-2 small-shaped) on your tokenized dataset. Periodically measure the validation loss on C4 100 domains (this is already enabled by default in the config at `cs336-basics/cs336_basics/train_config.py`). What is the best validation loss that your model achieves? Submit this value to the leaderboard.\n",
    "\n",
    "**Deliverable**: The best validation loss that was recorded, the associated learning curve, and a description of what you did.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: rgb(196, 196, 196); padding: 10px; color: #333;\">\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cd78e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
