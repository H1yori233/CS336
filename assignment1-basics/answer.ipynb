{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8260d6f5",
   "metadata": {},
   "source": [
    "## 2. Byte-Pair Encoding (BPE) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11868cba",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927397109025473129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36917f3a",
   "metadata": {},
   "source": [
    "### Problem (unicode1): Understanding Unicode (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b421fef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29275"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('牛')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d9dc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'牛'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(29275)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79eab4",
   "metadata": {},
   "source": [
    "(a) What Unicode character does chr(0) return?\n",
    "\n",
    "**Deliverable**: A one-sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4736cb3",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">null character</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d903c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87c7c6",
   "metadata": {},
   "source": [
    "(b) How does this character’s string representation (`__repr__()`) differ from its printed representation?\n",
    "\n",
    "**Deliverable**: A one-sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d744a1cc",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "The string representation (`__repr__()`) of the null character explicitly shows its escape sequence `'\\x00'`, whereas its printed representation typically appears as an empty string or nothing at all because it is a non-printable character.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2e55c",
   "metadata": {},
   "source": [
    "(c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations: \n",
    "\n",
    "```python\n",
    ">>> chr(0)\n",
    ">>> print(chr(0))\n",
    ">>> \"this is a test\" + chr(0) + \"string\"\n",
    ">>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "```\n",
    "\n",
    "**Deliverable**: A one-sentence response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096cff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "chr(0)\n",
    "print(chr(0))\n",
    "\"this is a test\" + chr(0) + \"string\"\n",
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88ae17",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "it is embedded within the string but is typically not visible when printed.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644003d",
   "metadata": {},
   "source": [
    "### Problem (unicode2): Unicode Encodings (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3bee5",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings.\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96790864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "<class 'bytes'>\n",
      "UTF-8 value:  104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33\n",
      "b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00'\n",
      "<class 'bytes'>\n",
      "UTF-16 value:  255, 254, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 33, 0, 32, 0, 83, 48, 147, 48, 107, 48, 97, 48, 111, 48, 33, 0\n",
      "b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00S0\\x00\\x00\\x930\\x00\\x00k0\\x00\\x00a0\\x00\\x00o0\\x00\\x00!\\x00\\x00\\x00'\n",
      "<class 'bytes'>\n",
      "UTF-32 value:  255, 254, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 33, 0, 0, 0, 32, 0, 0, 0, 83, 48, 0, 0, 147, 48, 0, 0, 107, 48, 0, 0, 97, 48, 0, 0, 111, 48, 0, 0, 33, 0, 0, 0\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(type(utf8_encoded))\n",
    "print(\"UTF-8 value: \", \", \".join(map(str, list(utf8_encoded))))\n",
    "utf16_encoded = test_string.encode(\"utf-16\")\n",
    "print(utf16_encoded)\n",
    "print(type(utf16_encoded))\n",
    "print(\"UTF-16 value: \", \", \".join(map(str, list(utf16_encoded))))\n",
    "utf32_encoded = test_string.encode(\"utf-32\")\n",
    "print(utf32_encoded)\n",
    "print(type(utf32_encoded))\n",
    "print(\"UTF-32 value: \", \", \".join(map(str, list(utf32_encoded))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b4e5a",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "UTF-8 is the most popular way and often more space-efficient\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6079a5d",
   "metadata": {},
   "source": [
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into\n",
    "a Unicode string. Why is this function incorrect? Provide an example of an input byte string\n",
    "that yields incorrect results.\n",
    "``` python\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "'hello'\n",
    "```\n",
    "\n",
    "**Deliverable**: An example input byte string for which decode_utf8_bytes_to_str_wrong pro-\n",
    "duces incorrect output, with a one-sentence explanation of why the function is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e394ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n\u001b[32m      3\u001b[39m test_string = \u001b[33m\"\u001b[39m\u001b[33mhello! こんにちは!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_string\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbytestring\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "test_string = \"hello! こんにちは!\"\n",
    "decode_utf8_bytes_to_str_wrong(test_string.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896e2f2",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "The function incorrectly attempts to decode each byte individually, leading to a UnicodeDecodeError because multi-byte UTF-8 characters cannot be decoded one byte at a time.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1db7bd",
   "metadata": {},
   "source": [
    "(c) Give a two byte sequence that does not decode to any Unicode character(s).\n",
    "\n",
    "**Deliverable**: An example, with a one-sentence explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3ff18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to decode: b'\\xc2\\x00'\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc2 in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m invalid_bytes = \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\xc2\u001b[39;00m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to decode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_bytes\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m decoded_string = \u001b[43minvalid_bytes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m decoded_string\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xc2 in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "invalid_bytes = b'\\xc2\\x00'\n",
    "print(f\"Attempting to decode: {invalid_bytes!r}\")\n",
    "decoded_string = invalid_bytes.decode(\"utf-8\")\n",
    "decoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2881a",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "b'\\xc2\\x00'\n",
    "because while 0xc2 is a valid start byte for a two-byte UTF-8 sequence, 0x00 is not a valid continuation byte.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05b077bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "# requires `regex` package\n",
    "import regex as re\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f2997",
   "metadata": {},
   "source": [
    "### Problem (train_bpe): BPE Tokenizer Training (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d758a",
   "metadata": {},
   "source": [
    "**Deliverable**: Write a function that, given a path to an input text file, trains a (byte-level) BPE\n",
    "tokenizer. Your BPE training function should handle (at least) the following input parameters:\n",
    "\n",
    "`input_path`: str Path to a text file with BPE tokenizer training data.\n",
    "\n",
    "`vocab_size`: int A positive integer that defines the maximum final vocabulary size (including the\n",
    "initial byte vocabulary, vocabulary items produced from merging, and any special tokens).\n",
    "\n",
    "`special_tokens`: list[str] A list of strings to add to the vocabulary. These special tokens do not\n",
    "otherwise affect BPE training.\n",
    "\n",
    "Your BPE training function should return the resulting vocabulary and merges:\n",
    "\n",
    "`vocab`: dict[int, bytes] The tokenizer vocabulary, a mapping from int (token ID in the vocabu-\n",
    "lary) to bytes (token bytes).\n",
    "\n",
    "`merges`: list[tuple[bytes, bytes]] A list of BPE merges produced from training. Each list item\n",
    "is a tuple of bytes (<token1>, <token2>), representing that <token1> was merged with\n",
    "<token2>. The merges should be ordered by order of creation.\n",
    "\n",
    "To test your BPE training function against our provided tests, you will first need to implement the\n",
    "test adapter at `[adapters.run_train_bpe]`. Then, run `uv run pytest tests/test_train_bpe.py`.\n",
    "Your implementation should be able to pass all tests. Optionally (this could be a large time-investment),\n",
    "you can implement the key parts of your training method using some systems language, for instance\n",
    "`C++` (consider `cppyy` for this) or `Rust` (using `PyO3`). If you do this, be aware of which operations\n",
    "require copying vs reading directly from Python memory, and make sure to leave build instructions, or\n",
    "make sure it builds using only `pyproject.toml`. Also note that the GPT-2 regex is not well-supported\n",
    "in most regex engines and will be too slow in most that do. We have verified that Oniguruma is\n",
    "reasonably fast and supports negative lookahead, but the `regex` package in Python is, if anything,\n",
    "even faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96961ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 3 items\n",
      "\n",
      "tests/test_train_bpe.py::test_train_bpe_speed \u001b[32mPASSED\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe \u001b[32mPASSED\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 2.56s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest tests/test_train_bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d95ab",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414daf3",
   "metadata": {},
   "source": [
    "Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size\n",
    "of 10,000. Make sure to add the TinyStories `<|endoftext|>` special token to the vocabulary.\n",
    "Serialize the resulting vocabulary and merges to disk for further inspection. How many hours\n",
    "and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n",
    "Resource requirements: ≤30 minutes (no GPUs), ≤ 30GB RAM\n",
    "\n",
    "**Hint** You should be able to get under 2 minutes for BPE training using multiprocessing during\n",
    "pretokenization and the following two facts:\n",
    "\n",
    "(a) The `<|endoftext|>` token delimits documents in the data files.\n",
    "\n",
    "(b) The `<|endoftext|>` token is handled as a special case before the BPE merges are applied.\n",
    "\n",
    "**Deliverable:** A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17854f8f",
   "metadata": {},
   "source": [
    "| uv run -m cs336_basics.tokenizer \"data/TinyStoriesV2-GPT4-train.txt\" --vocab_size 10000\n",
    "\n",
    "``` PowerShell\n",
    "==================================================\n",
    "🚀 Initializing BPETrainer...\n",
    "Configuration:\n",
    "  - Vocab Size: 10000\n",
    "  - Special Tokens: ['<|endoftext|>']\n",
    "  - Input File: data/TinyStoriesV2-GPT4-train.txt\n",
    "==================================================\n",
    "\n",
    "Step 1: Pre-tokenizing input file...\n",
    "✅ Pre-tokenization complete in 189.14 seconds.\n",
    "   Found 60006 unique pre-tokenized words/chunks.\n",
    "\n",
    "Step 2: Learning BPE merges...\n",
    "✅ Merging complete in 22.63 seconds.\n",
    "   Learned 9743 merges. Final vocab size: 10000\n",
    "   Longest token has 512 bytes.\n",
    "   Longest token content: 'ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss'\n",
    "\n",
    "Step 3: Saving vocabulary and merges...\n",
    "   Vocabulary saved to data/TinyStoriesV2-GPT4-train-vocab_size_10000-vocab.json\n",
    "   Merges saved to data/TinyStoriesV2-GPT4-train-vocab_size_10000-merges.txt\n",
    "\n",
    "🎉 Training complete!\n",
    "==================================================\n",
    "\n",
    "--- Resource Usage ---\n",
    "✅ Peak memory usage: 136.51 MB\n",
    "\n",
    "--- CProfile Performance Analysis (Top 20) ---\n",
    "         18090141 function calls (18090068 primitive calls) in 211.826 seconds\n",
    "\n",
    "   Ordered by: cumulative time\n",
    "   List reduced from 410 to 20 due to restriction <20>\n",
    "\n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        1    0.227    0.227  189.104  189.104 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:80(pretokenize)   \n",
    "        4    0.000    0.000  188.790   47.197 D:\\Tool\\Python311\\Lib\\threading.py:604(wait)\n",
    "        4    0.000    0.000  188.790   47.197 D:\\Tool\\Python311\\Lib\\threading.py:288(wait)\n",
    "       20  188.790    9.439  188.790    9.439 {method 'acquire' of '_thread.lock' objects}\n",
    "        1    0.000    0.000  188.789  188.789 D:\\Tool\\Python311\\Lib\\multiprocessing\\pool.py:362(map)\n",
    "        1    0.000    0.000  188.788  188.788 D:\\Tool\\Python311\\Lib\\multiprocessing\\pool.py:767(get)\n",
    "        1    0.000    0.000  188.788  188.788 D:\\Tool\\Python311\\Lib\\multiprocessing\\pool.py:764(wait)\n",
    "        1    5.626    5.626   22.232   22.232 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:153(merge)        \n",
    "   574642    5.532    0.000    9.659    0.000 {built-in method _heapq.heappop}\n",
    "   818506    4.868    0.000    6.453    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:187(update_stats) \n",
    " 11978239    4.527    0.000    4.527    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:30(__lt__)        \n",
    "   789060    0.810    0.000    1.210    0.000 {built-in method _heapq.heappush}\n",
    "    49746    0.353    0.000    0.353    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
    "        1    0.025    0.025    0.308    0.308 D:\\Tool\\Python311\\Lib\\json\\__init__.py:120(dump)\n",
    "   789060    0.226    0.000    0.226    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:24(__init__)      \n",
    "   784684    0.161    0.000    0.161    0.000 {method 'add' of 'set' objects}\n",
    "   293471    0.143    0.000    0.143    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:16(count)\n",
    "   574696    0.120    0.000    0.120    0.000 {method 'get' of 'dict' objects}\n",
    "   435323    0.082    0.000    0.082    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:10(__init__)      \n",
    "        1    0.000    0.000    0.081    0.081 D:\\Tool\\Python311\\Lib\\multiprocessing\\context.py:115(Pool)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e7315",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "The tokenizer was trained in approximately 3 minutes (189.14 seconds) with a peak memory usage of 136.51 MB, both well within the resource limits. The longest token, a 512-byte sequence of the repeating character 's'\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712493d9",
   "metadata": {},
   "source": [
    "Profile your code. What part of the tokenizer training process takes the most time?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec58ce",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "the pre-tokenization step (pretokenize function) is overwhelmingly the most time-consuming part of the training process, taking approximately 138 seconds.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd7318",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc079de",
   "metadata": {},
   "source": [
    "Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary\n",
    "size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What\n",
    "is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "**Resource requirements**: ≤12 hours (no GPUs), ≤ 100GB RAM\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e32d0a",
   "metadata": {},
   "source": [
    "| uv run -m cs336_basics.tokenizer \"data/owt_train.txt\" --vocab_size 32000\n",
    "\n",
    "``` PowerShell\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b47c18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "638596ed",
   "metadata": {},
   "source": [
    "Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31467b10",
   "metadata": {},
   "source": [
    "### Problem (tokenizer): Implementing the tokenizer (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4723663",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement a Tokenizer class that, given a vocabulary and a list of merges, encodes\n",
    "text into integer IDs and decodes integer IDs into text. Your tokenizer should also support user-provided\n",
    "special tokens (appending them to the vocabulary if they aren’t already there). We recommend the\n",
    "following interface:\n",
    "\n",
    "`def __init__(self, vocab, merges, special_tokens=None)` Construct a tokenizer from a given vocabulary, list of merges, and (optionally) a list of special tokens. This function should accept the following parameters:\n",
    "\n",
    "``` python\n",
    "vocab: dict[int, bytes]\n",
    "merges: list[tuple[bytes, bytes]]\n",
    "special_tokens: list[str] | None = None\n",
    "```\n",
    "\n",
    "`def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None)` Class method that constructs and return a Tokenizer from a serialized vocabulary and list of merges (in the same format that your BPE training code output) and (optionally) a list of special tokens. This method should accept the following additional parameters:\n",
    "\n",
    "``` python\n",
    "vocab_filepath: str\n",
    "merges_filepath: str\n",
    "special_tokens: list[str] | None = None\n",
    "```\n",
    "\n",
    "`def encode(self, text: str) -> list[int]` Encode an input text into a sequence of token IDs.\n",
    "\n",
    "`def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]` Given an iterable of strings (e.g., a Python file handle), return a generator that lazily yields token IDs. This is required for memory-eﬀicient tokenization of large files that we cannot directly load into memory.\n",
    "\n",
    "`def decode(self, ids: list[int]) -> str` Decode a sequence of token IDs into text.\n",
    "\n",
    "To test your Tokenizer against our provided tests, you will first need to implement the test adapter at `[adapters.get_tokenizer]`. Then, run `uv run pytest tests/test_tokenizer.py`. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de824fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 25 items\n",
      "\n",
      "tests/test_tokenizer.py::test_roundtrip_empty \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_empty_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_single_character \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_single_character_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_single_unicode_character \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_ascii_string \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_ascii_string_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_unicode_string \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_unicode_string_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_overlapping_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_address_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_address_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_german_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_german_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_tinystories_sample_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_tinystories_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_special_token_trailing_newlines \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_memory_usage \u001b[33mSKIPPED\u001b[0m (...)\n",
      "tests/test_tokenizer.py::test_encode_memory_usage \u001b[33mSKIPPED\u001b[0m (rlimit su...)\n",
      "\n",
      "\u001b[32m======================== \u001b[32m\u001b[1m23 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m\u001b[32m in 3.71s\u001b[0m\u001b[32m ========================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest tests/test_tokenizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cde36d",
   "metadata": {},
   "source": [
    "### Problem (tokenizer_experiments): Experiments with tokenizers (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f0e4f",
   "metadata": {},
   "source": [
    "Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30053fff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "270ba725",
   "metadata": {},
   "source": [
    "What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ccffe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f022dfce",
   "metadata": {},
   "source": [
    "Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd9550",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85748033",
   "metadata": {},
   "source": [
    "Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We’ll use this later to train our language model. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is uint16 an appropriate choice?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49b952",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd2d86f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f65a24",
   "metadata": {},
   "source": [
    "## 3. Transformer Language Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b7156",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927015802915263462\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "  <img src=\"data/transformer.png\" alt=\"Transformer Image\" style=\"width: 30%;\">\n",
    "  <img src=\"data/prenorm_transformer.png\" alt=\"Pre-Norm Transformer Image\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24826371",
   "metadata": {},
   "source": [
    "### Problem (linear): Implementing the linear module (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21033471",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement a `Linear` class that inherits from `torch.nn.Module` and performs a linear transformation. Your implementation should follow the interface of PyTorch’s built-in `nn.Linear` module, except for not having a bias argument or parameter. We recommend the following interface:\n",
    "\n",
    "`def __init__(self, in_features, out_features, device=None, dtype=None)` Construct a\n",
    "linear transformation module. This function should accept the following parameters:\n",
    "``` python\n",
    "in_features: int final dimension of the input\n",
    "out_features: int final dimension of the output\n",
    "device: torch.device | None = None Device to store the parameters on\n",
    "dtype: torch.dtype | None = None Data type of the parameters\n",
    "```\n",
    "\n",
    "`def forward(self, x: torch.Tensor) -> torch.Tensor` Apply the linear transformation to the input.\n",
    "\n",
    "Make sure to:\n",
    "\n",
    "- subclass `nn.Module`\n",
    "- call the superclass constructor\n",
    "- construct and store your parameter as $W$ (not $W^T$) for memory ordering reasons, putting it in an `nn.Parameter`\n",
    "- of course, don’t use `nn.Linear` or `nn.functional.linear`\n",
    "\n",
    "For initializations, use the settings from above along with `torch.nn.init.trunc_normal_` to\n",
    "initialize the weights.\n",
    "To test your Linear module, implement the test adapter at `[adapters.run_linear]`. The adapter\n",
    "should load the given weights into your Linear module. You can use `Module.load_state_dict` for\n",
    "this purpose. Then, run `uv run pytest -k test_linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb7c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_linear \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.55s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb0415",
   "metadata": {},
   "source": [
    "### Problem (embedding): Implement the embedding module (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5f11d",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement the `Embedding` class that inherits from `torch.nn.Module` and performs an\n",
    "embedding lookup. Your implementation should follow the interface of PyTorch’s built-in\n",
    "`nn.Embedding` module. We recommend the following interface:\n",
    "\n",
    "`def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None)` Construct an embedding module. This function should accept the following parameters:\n",
    "\n",
    "``` python\n",
    "num_embeddings: int Size of the vocabulary\n",
    "embedding_dim: int Dimension of the embedding vectors, i.e., d_model\n",
    "device: torch.device | None = None Device to store the parameters on\n",
    "dtype: torch.dtype | None = None Data type of the parameters\n",
    "```\n",
    "\n",
    "`def forward(self, token_ids: torch.Tensor) -> torch.Tensor` Lookup the embedding vectors for the given token IDs.\n",
    "\n",
    "Make sure to:\n",
    "- subclass nn.Module\n",
    "- call the superclass constructor\n",
    "- initialize your embedding matrix as a nn.Parameter\n",
    "- store the embedding matrix with the d_model being the final dimension\n",
    "- of course, don’t use nn.Embedding or nn.functional.embedding\n",
    "\n",
    "Again, use the settings from above for initialization, and use `torch.nn.init.trunc_normal_` to\n",
    "initialize the weights.\n",
    "\n",
    "To test your implementation, implement the test adapter at `[adapters.run_embedding]`. Then, run\n",
    "`uv run pytest -k test_embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f2866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_embedding \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.13s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81feed",
   "metadata": {},
   "source": [
    "### Problem (rmsnorm): Root Mean Square Layer Normalization (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887c53b",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement `RMSNorm` as a `torch.nn.Module`. We recommend the following interface:\n",
    "\n",
    "`def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None)` Construct the RMSNorm module. This function should accept the following parameters:\n",
    "\n",
    "``` python\n",
    "d_model: int Hidden dimension of the model\n",
    "eps: float = 1e-5 Epsilon value for numerical stability\n",
    "device: torch.device | None = None Device to store the parameters on\n",
    "dtype: torch.dtype | None = None Data type of the parameters\n",
    "```\n",
    "\n",
    "`def forward(self, x: torch.Tensor) -> torch.Tensor` Process an input tensor of shape (`batch_size`, `sequence_length`, `d_model`) and return a tensor of the same shape.\n",
    "\n",
    "**Note**: Remember to upcast your input to `torch.float32` before performing the normalization (and\n",
    "later downcast to the original `dtype`), as described above.\n",
    "\n",
    "To test your implementation, implement the test adapter at `[adapters.run_rmsnorm]`. Then, run `uv\n",
    "run pytest -k test_rmsnorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007bc701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_rmsnorm \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_rmsnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df4b20",
   "metadata": {},
   "source": [
    "### Problem (positionwise_feedforward): Implement the position-wise feed-forward network (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c92b55",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement the `SwiGLU` feed-forward network, composed of a `SiLU` activation function and a `GLU`.\n",
    "\n",
    "**Note**: in this particular case, you should feel free to use torch.sigmoid in your implementation for numerical stability.\n",
    "\n",
    "You should set dff to approximately $\\frac{8}{3}$ × $d_\\text{model}$ in your implementation, while ensuring that the dimensionality of the inner feed-forward layer is a multiple of 64 to make good use of your hardware. To test your implementation against our provided tests, you will need to implement\n",
    "the test adapter at `[adapters.run_swiglu]`. Then, run `uv run pytest -k test_swiglu` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c096783b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_swiglu \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.54s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_swiglu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c7b19",
   "metadata": {},
   "source": [
    "### Problem (rope): Implement RoPE (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca76b4e",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement a class `RotaryPositionalEmbedding` that applies RoPE to the input tensor.\n",
    "\n",
    "The following interface is recommended:\n",
    "\n",
    "`def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None)` Construct the\n",
    "RoPE module and create buffers if needed.\n",
    "\n",
    "``` python\n",
    "theta: float Θ value for the RoPE\n",
    "d_k: int dimension of query and key vectors\n",
    "max_seq_len: int Maximum sequence length that will be inputted\n",
    "device: torch.device | None = None Device to store the buffer on\n",
    "```\n",
    "\n",
    "`def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor` Process an input tensor of shape `(..., seq_len, d_k)` and return a tensor of the same shape. Note that you should tolerate x with an arbitrary number of batch dimensions. You should assume that the token positions are a tensor of shape `(..., seq_len)` specifying the token positions of `x` along the sequence dimension.\n",
    "\n",
    "You should use the token positions to slice your (possibly precomputed) cos and sin tensors along the sequence dimension.\n",
    "\n",
    "To test your implementation, complete [adapters.run_rope] and make sure it passes `uv run pytest -k test_rope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a43ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_rope \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.14s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_rope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce930c",
   "metadata": {},
   "source": [
    "### Problem (softmax): Implement softmax (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe33778",
   "metadata": {},
   "source": [
    "**Deliverable**: Write a function to apply the `softmax` operation on a tensor. Your function should take two parameters: a tensor and a dimension $i$, and apply softmax to the $i$-th dimension of the input tensor. The output tensor should have the same shape as the input tensor, but its $i$-th dimension will now have a normalized probability distribution. Use the trick of subtracting the maximum value in the $i$-th dimension from all elements of the $i$-th dimension to avoid numerical stability issues.\n",
    "\n",
    "To test your implementation, complete `[adapters.run_softmax]` and make sure it passes `uv run pytest -k test_softmax_matches_pytorch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae1a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_nn_utils.py::test_softmax_matches_pytorch \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_softmax_matches_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74401631",
   "metadata": {},
   "source": [
    "### Problem (scaled_dot_product_attention): Implement scaled dot-product attention (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe00ff",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement the scaled dot-product attention function. Your implementation should handle keys and queries of shape `(batch_size, ..., seq_len, d_k)` and values of shape `(batch_size, ..., seq_len, d_v)`, where `...` represents any number of other batch-like dimensions (if provided). The implementation should return an output with the shape `(batch_size, ..., d_v)`. See section $3.3$ for a discussion on batch-like dimensions.\n",
    "\n",
    "Your implementation should also support an optional user-provided boolean mask of shape `(seq_len, seq_len)`. The attention probabilities of positions with a mask value of `True` should collectively sum\n",
    "to $1$, and the attention probabilities of positions with a mask value of `False` should be zero.\n",
    "To test your implementation against our provided tests, you will need to implement the test adapter\n",
    "at `[adapters.run_scaled_dot_product_attention]`.\n",
    "\n",
    "`uv run pytest -k test_scaled_dot_product_attention` tests your implementation on third-order input tensors, while `uv run pytest -k test_4d_scaled_dot_product_attention` tests your implementation on fourth-order input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c20af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_scaled_dot_product_attention \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.49s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248483ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_4d_scaled_dot_product_attention \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.58s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_4d_scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e58f1b",
   "metadata": {},
   "source": [
    "### Problem (multihead_self_attention): Implement causal multi-head self-attention (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233c4ea",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement causal multi-head self-attention as a `torch.nn.Module`. Your implementation should accept (at least) the following parameters:\n",
    "\n",
    "``` python\n",
    "d_model: int Dimensionality of the Transformer block inputs.\n",
    "num_heads: int Number of heads to use in multi-head self-attention.\n",
    "```\n",
    "\n",
    "Folllowing $\\text{Vaswani et al. [2017]}$, set $d_k=d_v=\\frac{d_{model}}h$. To test your implementation against our provided tests, implement the test adapter at `[adapters.run_multihead_self_attention]`. Then, run `uv run pytest -k test_multihead_self_attention` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf6a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 46 deselected / 2 selected\n",
      "\n",
      "tests/test_model.py::test_multihead_self_attention \u001b[32mPASSED\u001b[0m\n",
      "tests/test_model.py::test_multihead_self_attention_with_rope \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m46 deselected\u001b[0m\u001b[32m in 1.51s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_multihead_self_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414dba8",
   "metadata": {},
   "source": [
    "### Problem (transformer_block): Implement the Transformer block (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8755a5",
   "metadata": {},
   "source": [
    "Implement the pre-norm Transformer block as described in $§3.5$ and illustrated in $\\text{Figure} 2$. Your\n",
    "Transformer block should accept (at least) the following parameters.\n",
    "\n",
    "``` python\n",
    "d_model: int Dimensionality of the Transformer block inputs.\n",
    "num_heads: int Number of heads to use in multi-head self-attention.\n",
    "d_ff: int Dimensionality of the position-wise feed-forward inner layer.\n",
    "```\n",
    "\n",
    "To test your implementation, implement the adapter `[adapters.run_transformer_block]`. Then run `uv run pytest -k test_transformer_block` to test your implementation.\n",
    "\n",
    "**Deliverable**: Transformer block code that passes the provided tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0611eb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_transformer_block \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.53s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_transformer_block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888d755",
   "metadata": {},
   "source": [
    "### Problem (transformer_lm): Implementing the Transformer LM (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fb709",
   "metadata": {},
   "source": [
    "Time to put it all together! Implement the Transformer language model as described in $§3.1$ and illustrated in $\\text{Figure} 1$. At minimum, your implementation should accept all the aforementioned construction parameters for the Transformer block, as well as these additional parameters:\n",
    "\n",
    "``` python\n",
    "vocab_size: int The size of the vocabulary, necessary for determining the dimensionality of the token embedding matrix.\n",
    "context_length: int The maximum context length, necessary for determining the dimensionality of the position embedding matrix.\n",
    "num_layers: int The number of Transformer blocks to use.\n",
    "```\n",
    "\n",
    "To test your implementation against our provided tests, you will first need to implement the test adapter at `[adapters.run_transformer_lm]`. Then, run `uv run pytest -k test_transformer_lm` to test your implementation.\n",
    "\n",
    "**Deliverable**: A Transformer LM module that passes the above tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f5a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 46 deselected / 2 selected\n",
      "\n",
      "tests/test_model.py::test_transformer_lm \u001b[32mPASSED\u001b[0m\n",
      "tests/test_model.py::test_transformer_lm_truncated_input \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m46 deselected\u001b[0m\u001b[32m in 1.56s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_transformer_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89a0c7",
   "metadata": {},
   "source": [
    "### Problem (transformer_accounting): Transformer LM resource accounting (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5fde5",
   "metadata": {},
   "source": [
    "Consider GPT-2 XL, which has the following configuration:\n",
    "\n",
    "``` python\n",
    "vocab_size : 50,257\n",
    "context_length : 1,024\n",
    "num_layers : 48\n",
    "d_model : 1,600\n",
    "num_heads : 25\n",
    "d_ff : 6,400\n",
    "```\n",
    "\n",
    "Suppose we constructed our model using this configuration. How many trainable parameters\n",
    "would our model have? Assuming each parameter is represented using single-precision floating\n",
    "point, how much memory is required to just load this model?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545886c",
   "metadata": {},
   "source": [
    "``` markdown\n",
    "**token_embeddings**: vocab_size * d_model = 50,257 * 1,600 = 80,411,200\n",
    "**note**: embedding is is a **lookup**, but it is mathematically equivalent to a **matrix multiplication** with a **one-hot encoded vector**.\n",
    "\n",
    "**layers**:\n",
    "- **attn**:\n",
    "- - q_proj: d_model × d_model = 1,600 × 1,600 = 2,560,000\n",
    "- - k_proj: d_model × d_model = 1,600 × 1,600 = 2,560,000\n",
    "- - v_proj: d_model × d_model = 1,600 × 1,600 = 2,560,000\n",
    "- - output_proj: d_model × d_model = 1,600 × 1,600 = 2,560,000\n",
    "- **ffn**:\n",
    "- - w1: d_model × d_ff = 1,600 × 6,400 = 10,240,000\n",
    "- - w2: d_ff × d_model = 6,400 × 1,600 = 10,240,000\n",
    "- - w3: d_model × d_ff = 1,600 × 6,400 = 10,240,000\n",
    "- **ln1**: d_model = 1600\n",
    "- **ln2**: d_model = 1600\n",
    "- 40,963,200 x num_layers = 40,963,200 x 48 = 1,966,233,600\n",
    "\n",
    "**ln_final**：d_model = 1,600\n",
    "\n",
    "**lm_head**: vocab_size * d_model = 50,257 * 1,600 = 80,411,200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9896f",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "2,127,058,600 parameters, × 4 bytes = 7.92 GB\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3336b3d",
   "metadata": {},
   "source": [
    "Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped\n",
    "model. How many `FLOPs` do these matrix multiplies require in total? Assume that our input\n",
    "sequence has context_length tokens.\n",
    "\n",
    "**Deliverable**: A list of matrix multiplies (with descriptions), and the total number of `FLOPs`\n",
    "required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3c752",
   "metadata": {},
   "source": [
    "``` markdown\n",
    "**Transformer Block**\n",
    "- MHA\n",
    "- - q_proj: (1024, 1600) × (1600, 1600) → FLOPs: 2 × 1024 × 1600 × 1600 = 5,242,880,000\n",
    "- - k_proj: (1024, 1600) × (1600, 1600) → FLOPs: 2 × 1024 × 1600 × 1600 = 5,242,880,000\n",
    "- - v_proj: (1024, 1600) × (1600, 1600) → FLOPs: 2 × 1024 × 1600 × 1600 = 5,242,880,000\n",
    "- - attention scores: (25, 1024, 64) × (25, 64, 1024) → FLOPs: 2 × 25 × 1024 × 64 × 1024 = 3,355,443,200\n",
    "- - attention: (25, 1024, 1024) × (25, 1024, 64) → FLOPs: 2 × 25 × 1024 × 1024 × 64 = 3,355,443,200\n",
    "- - output_proj: (1024, 1600) × (1600, 1600) → FLOPs: 2 × 1024 × 1600 × 1600 = 5,242,880,000\n",
    "- FFN\n",
    "- - w1: (1024, 1600) × (1600, 6400) → FLOPs: 2 × 1024 × 1600 × 6400 = 20,971,520,000\n",
    "- - w3: (1024, 1600) × (1600, 6400) → FLOPs: 2 × 1024 × 1600 × 6400 = 20,971,520,000\n",
    "- - w2: (1024, 6400) × (6400, 1600) → FLOPs: 2 × 1024 × 6400 × 1600 = 20,971,520,000\n",
    "- 90,596,966,400 * 48 = 4,348,654,387,200\n",
    "\n",
    "**LM Head**: (1024, 1600) × (1600, 50257) → FLOPs: 2 × 1024 × 1600 × 50257 = 164,681,932,800\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe4e74",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "4,513,297,920,000 FLOPs\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55999f",
   "metadata": {},
   "source": [
    "Based on your analysis above, which parts of the model require the most `FLOPs`?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d7fc6",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "FFN, as it projects the hidden state from d_model to a larger intermediate dimension d_ff (4x in this case), and then projects it back, which involves matrix multiplication operations much larger than the self attention part.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1c74d",
   "metadata": {},
   "source": [
    "Repeat your analysis with GPT-2 small (12 layers, 768 d_model, 12 heads), GPT-2 medium (24\n",
    "layers, 1024 d_model, 16 heads), and GPT-2 large (36 layers, 1280 d_model, 20 heads). As the\n",
    "model size increases, which parts of the Transformer LM take up proportionally more or less of\n",
    "the total `FLOPs`?\n",
    "\n",
    "**Deliverable**: For each model, provide a breakdown of model components and its associated\n",
    "`FLOPs` (as a proportion of the total `FLOPs` required for a forward pass). In addition, provide a\n",
    "one-to-two sentence description of how varying the model size changes the proportional `FLOPs`\n",
    "of each component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b244e",
   "metadata": {},
   "source": [
    "| model            | d_model (d) | FFN FLOPs         | Attention FLOPs | FFN %  | Attention % |\n",
    "| ---------------- | ----------- | ----------------- | --------------- | ------ | ----------- |\n",
    "| **GPT-2 small**  | 768         | 11,509,596,160    | 4,731,174,912   | ~64.3% | ~35.7%      |\n",
    "| **GPT-2 medium** | 1024        | 25,769,803,776    | 12,884,901,888  | ~66.7% | ~33.3%      |\n",
    "| **GPT-2 large**  | 1280        | 40,265,318,400    | 18,790,481,920  | ~68.2% | ~31.8%      |\n",
    "| **GPT-2 XL**     | 1600        | 62,914,560,000    | 27,682,406,400  | ~69.4% | ~30.6%      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be75617",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "The proportion of FFN in total FLOPs will relatively increase, while the proportion of self attention will relatively decrease\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a93bc",
   "metadata": {},
   "source": [
    "Take GPT-2 XL and increase the context length to 16,384. How does the total `FLOPs` for one\n",
    "forward pass change? How do the relative contribution of `FLOPs` of the model components\n",
    "change?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e6b2f6",
   "metadata": {},
   "source": [
    "``` markdown\n",
    "**Attention** increase sequence length dimension by 16 times, FLOPs increase by 16 times\n",
    "**FFN**: Increase sequence length dimension by 16 times, FLOPs increase by 16 times\n",
    "**Total**L FLOPs increase by approximately 16 times\n",
    "\n",
    "**FFN** still dominates, but the proportion of sequence length related calculations (Q × K and attention × V) for attention will slightly increase, as the complexity of these operations is O(n^2), while other operations are O(n).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c49d52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ad770",
   "metadata": {},
   "source": [
    "## 4. Training a Transformer LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac309850",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927132746578366957"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2119e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d283b2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4debc8fa",
   "metadata": {},
   "source": [
    "## 5. Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02176d0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8ba24ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710949",
   "metadata": {},
   "source": [
    "## 6. Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8d09f",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927311888515073257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c81cb6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "799684cc",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dec8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de6c9f6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
