{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8260d6f5",
   "metadata": {},
   "source": [
    "## 2. Byte-Pair Encoding (BPE) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11868cba",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927397109025473129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36917f3a",
   "metadata": {},
   "source": [
    "### Problem (unicode1): Understanding Unicode (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b421fef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29275"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('牛')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d9dc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'牛'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(29275)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79eab4",
   "metadata": {},
   "source": [
    "(a) What Unicode character does chr(0) return?\n",
    "\n",
    "**Deliverable**: A one-sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4736cb3",
   "metadata": {},
   "source": [
    "<span style=\"background-color: blue;\">null character</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d903c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87c7c6",
   "metadata": {},
   "source": [
    "(b) How does this character’s string representation (`__repr__()`) differ from its printed representation?\n",
    "\n",
    "**Deliverable**: A one-sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d744a1cc",
   "metadata": {},
   "source": [
    "<span style=\"background-color: blue;\">\n",
    "The string representation (`__repr__()`) of the null character explicitly shows its escape sequence `'\\x00'`, whereas its printed representation typically appears as an empty string or nothing at all because it is a non-printable character.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2e55c",
   "metadata": {},
   "source": [
    "(c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations: \n",
    "\n",
    "```python\n",
    ">>> chr(0)\n",
    ">>> print(chr(0))\n",
    ">>> \"this is a test\" + chr(0) + \"string\"\n",
    ">>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "```\n",
    "\n",
    "**Deliverable**: A one-sentence response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096cff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "chr(0)\n",
    "print(chr(0))\n",
    "\"this is a test\" + chr(0) + \"string\"\n",
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88ae17",
   "metadata": {},
   "source": [
    "<span style=\"background-color: blue;\">it is embedded within the string but is typically not visible when printed.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644003d",
   "metadata": {},
   "source": [
    "### Problem (unicode2): Unicode Encodings (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3bee5",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings.\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96790864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "<class 'bytes'>\n",
      "UTF-8 value:  104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33\n",
      "b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00'\n",
      "<class 'bytes'>\n",
      "UTF-16 value:  255, 254, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 33, 0, 32, 0, 83, 48, 147, 48, 107, 48, 97, 48, 111, 48, 33, 0\n",
      "b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00S0\\x00\\x00\\x930\\x00\\x00k0\\x00\\x00a0\\x00\\x00o0\\x00\\x00!\\x00\\x00\\x00'\n",
      "<class 'bytes'>\n",
      "UTF-32 value:  255, 254, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 33, 0, 0, 0, 32, 0, 0, 0, 83, 48, 0, 0, 147, 48, 0, 0, 107, 48, 0, 0, 97, 48, 0, 0, 111, 48, 0, 0, 33, 0, 0, 0\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(type(utf8_encoded))\n",
    "print(\"UTF-8 value: \", \", \".join(map(str, list(utf8_encoded))))\n",
    "utf16_encoded = test_string.encode(\"utf-16\")\n",
    "print(utf16_encoded)\n",
    "print(type(utf16_encoded))\n",
    "print(\"UTF-16 value: \", \", \".join(map(str, list(utf16_encoded))))\n",
    "utf32_encoded = test_string.encode(\"utf-32\")\n",
    "print(utf32_encoded)\n",
    "print(type(utf32_encoded))\n",
    "print(\"UTF-32 value: \", \", \".join(map(str, list(utf32_encoded))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b4e5a",
   "metadata": {},
   "source": [
    "<span style=\"background-color: blue;\">UTF-8 is the most popular way and often more space-efficient</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6079a5d",
   "metadata": {},
   "source": [
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into\n",
    "a Unicode string. Why is this function incorrect? Provide an example of an input byte string\n",
    "that yields incorrect results.\n",
    "``` python\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "'hello'\n",
    "```\n",
    "\n",
    "**Deliverable**: An example input byte string for which decode_utf8_bytes_to_str_wrong pro-\n",
    "duces incorrect output, with a one-sentence explanation of why the function is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e394ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n\u001b[32m      3\u001b[39m test_string = \u001b[33m\"\u001b[39m\u001b[33mhello! こんにちは!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_string\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbytestring\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "test_string = \"hello! こんにちは!\"\n",
    "decode_utf8_bytes_to_str_wrong(test_string.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896e2f2",
   "metadata": {},
   "source": [
    "<span style=\"background-color: blue;\">\n",
    "The function incorrectly attempts to decode each byte individually, leading to a UnicodeDecodeError because multi-byte UTF-8 characters cannot be decoded one byte at a time.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1db7bd",
   "metadata": {},
   "source": [
    "(c) Give a two byte sequence that does not decode to any Unicode character(s).\n",
    "\n",
    "**Deliverable**: An example, with a one-sentence explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3ff18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to decode: b'\\xc2\\x00'\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc2 in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m invalid_bytes = \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\xc2\u001b[39;00m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to decode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_bytes\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m decoded_string = \u001b[43minvalid_bytes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m decoded_string\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xc2 in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "invalid_bytes = b'\\xc2\\x00'\n",
    "print(f\"Attempting to decode: {invalid_bytes!r}\")\n",
    "decoded_string = invalid_bytes.decode(\"utf-8\")\n",
    "decoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2881a",
   "metadata": {},
   "source": [
    "<span style=\"background-color: blue;\">\n",
    "b'\\xc2\\x00'\n",
    "because while 0xc2 is a valid start byte for a two-byte UTF-8 sequence, 0x00 is not a valid continuation byte.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05b077bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "# requires `regex` package\n",
    "import regex as re\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f2997",
   "metadata": {},
   "source": [
    "### Problem (train_bpe): BPE Tokenizer Training (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d758a",
   "metadata": {},
   "source": [
    "**Deliverable**: Write a function that, given a path to an input text file, trains a (byte-level) BPE\n",
    "tokenizer. Your BPE training function should handle (at least) the following input parameters:\n",
    "\n",
    "`input_path`: str Path to a text file with BPE tokenizer training data.\n",
    "\n",
    "`vocab_size`: int A positive integer that defines the maximum final vocabulary size (including the\n",
    "initial byte vocabulary, vocabulary items produced from merging, and any special tokens).\n",
    "\n",
    "`special_tokens`: list[str] A list of strings to add to the vocabulary. These special tokens do not\n",
    "otherwise affect BPE training.\n",
    "\n",
    "Your BPE training function should return the resulting vocabulary and merges:\n",
    "\n",
    "`vocab`: dict[int, bytes] The tokenizer vocabulary, a mapping from int (token ID in the vocabu-\n",
    "lary) to bytes (token bytes).\n",
    "\n",
    "`merges`: list[tuple[bytes, bytes]] A list of BPE merges produced from training. Each list item\n",
    "is a tuple of bytes (<token1>, <token2>), representing that <token1> was merged with\n",
    "<token2>. The merges should be ordered by order of creation.\n",
    "\n",
    "To test your BPE training function against our provided tests, you will first need to implement the\n",
    "test adapter at `[adapters.run_train_bpe]`. Then, run `uv run pytest tests/test_train_bpe.py`.\n",
    "Your implementation should be able to pass all tests. Optionally (this could be a large time-investment),\n",
    "you can implement the key parts of your training method using some systems language, for instance\n",
    "`C++` (consider `cppyy` for this) or `Rust` (using `PyO3`). If you do this, be aware of which operations\n",
    "require copying vs reading directly from Python memory, and make sure to leave build instructions, or\n",
    "make sure it builds using only `pyproject.toml`. Also note that the GPT-2 regex is not well-supported\n",
    "in most regex engines and will be too slow in most that do. We have verified that Oniguruma is\n",
    "reasonably fast and supports negative lookahead, but the `regex` package in Python is, if anything,\n",
    "even faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96961ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 3 items\n",
      "\n",
      "tests/test_train_bpe.py::test_train_bpe_speed \u001b[32mPASSED\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe \u001b[32mPASSED\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe_special_tokens \u001b[31mFAILED\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________ test_train_bpe_special_tokens ________________________\u001b[0m\n",
      "\n",
      "snapshot = <tests.conftest.Snapshot object at 0x0000024489ED3710>\n",
      "\n",
      "    def test_train_bpe_special_tokens(snapshot):\n",
      "        \"\"\"\n",
      "        Ensure that the special tokens are added to the vocabulary and not\n",
      "        merged with other tokens.\n",
      "        \"\"\"\n",
      "        input_path = FIXTURES_PATH / \"tinystories_sample_5M.txt\"\n",
      "        vocab, merges = run_train_bpe(\n",
      "            input_path=input_path,\n",
      "            vocab_size=1000,\n",
      "            special_tokens=[\"<|endoftext|>\"],\n",
      "        )\n",
      "    \n",
      "        # Check that the special token is not in the vocab\n",
      "        vocabs_without_specials = [word for word in vocab.values() if word != b\"<|endoftext|>\"]\n",
      "        for word_bytes in vocabs_without_specials:\n",
      "            assert b\"<|\" not in word_bytes\n",
      "    \n",
      ">       snapshot.assert_match(\n",
      "            {\n",
      "                \"vocab_keys\": set(vocab.keys()),\n",
      "                \"vocab_values\": set(vocab.values()),\n",
      "                \"merges\": merges,\n",
      "            },\n",
      "        )\n",
      "\n",
      "\u001b[1m\u001b[31mtests\\test_train_bpe.py\u001b[0m:82: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\u001b[1m\u001b[31mtests\\conftest.py\u001b[0m:146: in patched_assert_match\n",
      "    return original_assert_match(actual, test_name=test_name, force_update=force_update)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "self = <tests.conftest.Snapshot object at 0x0000024489ED3710>\n",
      "actual = {'merges': [(b' ', b't'), (b'h', b'e'), (b' ', b'a'), (b' ', b's'), (b' ', b'w'), (b'n', b'd'), ...], 'vocab_keys': {0, 1, 2, 3, 4, 5, ...}, 'vocab_values': {b'\\x00', b'\\x01', b'\\x02', b'\\x03', b'\\x04', b'\\x05', ...}}\n",
      "test_name = 'test_train_bpe_special_tokens', force_update = False\n",
      "\n",
      "    def assert_match(\n",
      "        self,\n",
      "        actual: _A | dict[str, _A],\n",
      "        test_name: str,\n",
      "        force_update: bool = False,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Assert that the actual data matches the snapshot.\n",
      "        Args:\n",
      "            actual: Single object or dictionary of named objects\n",
      "            test_name: The name of the test (used for the snapshot file)\n",
      "            force_update: If True, update the snapshot instead of comparing\n",
      "        \"\"\"\n",
      "    \n",
      "        snapshot_path = self._get_snapshot_path(test_name)\n",
      "    \n",
      "    \n",
      "        # Load the snapshot\n",
      "        with open(snapshot_path, \"rb\") as f:\n",
      "            expected_data = pickle.load(f)\n",
      "    \n",
      "        if isinstance(actual, dict):\n",
      "            for key in actual:\n",
      "                if key not in expected_data:\n",
      "                    raise AssertionError(f\"Key '{key}' not found in snapshot for {test_name}\")\n",
      ">               assert actual[key] == expected_data[key], f\"Data for key '{key}' does not match snapshot for {test_name}\"\n",
      "\u001b[1m\u001b[31mE               AssertionError: Data for key 'vocab_values' does not match snapshot for test_train_bpe_special_tokens\u001b[0m\n",
      "\u001b[1m\u001b[31mE               assert {b'\\x00', b'\\... b'\\x05', ...} == {b'\\x00', b'\\... b'\\x05', ...}\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 \u001b[0m\n",
      "\u001b[1m\u001b[31mE                 Extra items in the left set:\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 b' bet'\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 b' ad'\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 b' start'\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 b'raw'\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 b'ane'...\u001b[0m\n",
      "\u001b[1m\u001b[31mE                 \u001b[0m\n",
      "\u001b[1m\u001b[31mE                 ...Full output truncated (1053 lines hidden), use '-vv' to show\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests\\conftest.py\u001b[0m:119: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_train_bpe.py::\u001b[1mtest_train_bpe_special_tokens\u001b[0m - AssertionError: Data for key 'vocab_values' does not match snapshot for tes...\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 3.94s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest tests/test_train_bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d95ab",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414daf3",
   "metadata": {},
   "source": [
    "Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size\n",
    "of 10,000. Make sure to add the TinyStories `<|endoftext|>` special token to the vocabulary.\n",
    "Serialize the resulting vocabulary and merges to disk for further inspection. How many hours\n",
    "and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n",
    "Resource requirements: ≤30 minutes (no GPUs), ≤ 30GB RAM\n",
    "\n",
    "**Hint** You should be able to get under 2 minutes for BPE training using multiprocessing during\n",
    "pretokenization and the following two facts:\n",
    "\n",
    "(a) The `<|endoftext|>` token delimits documents in the data files.\n",
    "\n",
    "(b) The `<|endoftext|>` token is handled as a special case before the BPE merges are applied.\n",
    "\n",
    "**Deliverable:** A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17854f8f",
   "metadata": {},
   "source": [
    "| uv run -m cs336_basics.tokenizer \"data/TinyStoriesV2-GPT4-train.txt\" --vocab_size 10000\n",
    "\n",
    "``` PowerShell\n",
    "==================================================\n",
    "🚀 Initializing BPETrainer...\n",
    "Configuration:\n",
    "  - Vocab Size: 10000\n",
    "  - Special Tokens: ['<|endoftext|>']\n",
    "  - Input File: data/TinyStoriesV2-GPT4-train.txt\n",
    "==================================================\n",
    "\n",
    "Step 1: Pre-tokenizing input file...\n",
    "✅ Pre-tokenization complete in 189.14 seconds.\n",
    "   Found 60006 unique pre-tokenized words/chunks.\n",
    "\n",
    "Step 2: Learning BPE merges...\n",
    "✅ Merging complete in 22.63 seconds.\n",
    "   Learned 9743 merges. Final vocab size: 10000\n",
    "   Longest token has 512 bytes.\n",
    "   Longest token content: 'ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss'\n",
    "\n",
    "Step 3: Saving vocabulary and merges...\n",
    "   Vocabulary saved to data/TinyStoriesV2-GPT4-train-vocab_size_10000-vocab.json\n",
    "   Merges saved to data/TinyStoriesV2-GPT4-train-vocab_size_10000-merges.txt\n",
    "\n",
    "🎉 Training complete!\n",
    "==================================================\n",
    "\n",
    "--- Resource Usage ---\n",
    "✅ Peak memory usage: 136.51 MB\n",
    "\n",
    "--- CProfile Performance Analysis (Top 20) ---\n",
    "         18090141 function calls (18090068 primitive calls) in 211.826 seconds\n",
    "\n",
    "   Ordered by: cumulative time\n",
    "   List reduced from 410 to 20 due to restriction <20>\n",
    "\n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        1    0.227    0.227  189.104  189.104 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:80(pretokenize)   \n",
    "        4    0.000    0.000  188.790   47.197 D:\\Tool\\Python311\\Lib\\threading.py:604(wait)\n",
    "        4    0.000    0.000  188.790   47.197 D:\\Tool\\Python311\\Lib\\threading.py:288(wait)\n",
    "       20  188.790    9.439  188.790    9.439 {method 'acquire' of '_thread.lock' objects}\n",
    "        1    0.000    0.000  188.789  188.789 D:\\Tool\\Python311\\Lib\\multiprocessing\\pool.py:362(map)\n",
    "        1    0.000    0.000  188.788  188.788 D:\\Tool\\Python311\\Lib\\multiprocessing\\pool.py:767(get)\n",
    "        1    0.000    0.000  188.788  188.788 D:\\Tool\\Python311\\Lib\\multiprocessing\\pool.py:764(wait)\n",
    "        1    5.626    5.626   22.232   22.232 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:153(merge)        \n",
    "   574642    5.532    0.000    9.659    0.000 {built-in method _heapq.heappop}\n",
    "   818506    4.868    0.000    6.453    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:187(update_stats) \n",
    " 11978239    4.527    0.000    4.527    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:30(__lt__)        \n",
    "   789060    0.810    0.000    1.210    0.000 {built-in method _heapq.heappush}\n",
    "    49746    0.353    0.000    0.353    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
    "        1    0.025    0.025    0.308    0.308 D:\\Tool\\Python311\\Lib\\json\\__init__.py:120(dump)\n",
    "   789060    0.226    0.000    0.226    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:24(__init__)      \n",
    "   784684    0.161    0.000    0.161    0.000 {method 'add' of 'set' objects}\n",
    "   293471    0.143    0.000    0.143    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:16(count)\n",
    "   574696    0.120    0.000    0.120    0.000 {method 'get' of 'dict' objects}\n",
    "   435323    0.082    0.000    0.082    0.000 E:\\Code\\CS336\\assignment1-basics\\cs336_basics\\tokenizer.py:10(__init__)      \n",
    "        1    0.000    0.000    0.081    0.081 D:\\Tool\\Python311\\Lib\\multiprocessing\\context.py:115(Pool)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e7315",
   "metadata": {},
   "source": [
    "<span style=\"background-color: blue;\">\n",
    "The tokenizer was trained in approximately 3 minutes (189.14 seconds) with a peak memory usage of 136.51 MB, both well within the resource limits. The longest token, a 512-byte sequence of the repeating character 's'\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712493d9",
   "metadata": {},
   "source": [
    "Profile your code. What part of the tokenizer training process takes the most time?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec58ce",
   "metadata": {},
   "source": [
    "<span style=\"background-color: blue;\">\n",
    "the pre-tokenization step (pretokenize function) is overwhelmingly the most time-consuming part of the training process, taking approximately 138 seconds.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd7318",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc079de",
   "metadata": {},
   "source": [
    "Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary\n",
    "size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What\n",
    "is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "**Resource requirements**: ≤12 hours (no GPUs), ≤ 100GB RAM\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e32d0a",
   "metadata": {},
   "source": [
    "| uv run -m cs336_basics.tokenizer \"data/owt_train.txt\" --vocab_size 32000\n",
    "\n",
    "``` PowerShell\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b47c18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "638596ed",
   "metadata": {},
   "source": [
    "Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190f3ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31467b10",
   "metadata": {},
   "source": [
    "### Problem (tokenizer): Implementing the tokenizer (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4723663",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement a Tokenizer class that, given a vocabulary and a list of merges, encodes\n",
    "text into integer IDs and decodes integer IDs into text. Your tokenizer should also support user-provided\n",
    "special tokens (appending them to the vocabulary if they aren’t already there). We recommend the\n",
    "following interface:\n",
    "\n",
    "`def __init__(self, vocab, merges, special_tokens=None)` Construct a tokenizer from a given vocabulary, list of merges, and (optionally) a list of special tokens. This function should accept the following parameters:\n",
    "\n",
    "|vocab: dict[int, bytes]\n",
    "\n",
    "|merges: list[tuple[bytes, bytes]]\n",
    "\n",
    "|special_tokens: list[str] | None = None\n",
    "\n",
    "`def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None)` Class method that constructs and return a Tokenizer from a serialized vocabulary and list of merges (in the same format that your BPE training code output) and (optionally) a list of special tokens. This method should accept the following additional parameters:\n",
    "\n",
    "|vocab_filepath: str\n",
    "\n",
    "|merges_filepath: str\n",
    "\n",
    "|special_tokens: list[str] | None = None\n",
    "\n",
    "`def encode(self, text: str) -> list[int]` Encode an input text into a sequence of token IDs.\n",
    "\n",
    "`def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]` Given an iterable of strings (e.g., a Python file handle), return a generator that lazily yields token IDs. This is required for memory-eﬀicient tokenization of large files that we cannot directly load into memory.\n",
    "\n",
    "`def decode(self, ids: list[int]) -> str` Decode a sequence of token IDs into text.\n",
    "\n",
    "To test your Tokenizer against our provided tests, you will first need to implement the test adapter at `[adapters.get_tokenizer]`. Then, run `uv run pytest tests/test_tokenizer.py`. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de824fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 25 items\n",
      "\n",
      "tests/test_tokenizer.py::test_roundtrip_empty \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_empty_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_single_character \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_single_character_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_single_unicode_character \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_ascii_string \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_ascii_string_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_unicode_string \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_unicode_string_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_overlapping_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_address_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_address_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_german_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_german_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_tinystories_sample_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_tinystories_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_special_token_trailing_newlines \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_memory_usage \u001b[33mSKIPPED\u001b[0m (...)\n",
      "tests/test_tokenizer.py::test_encode_memory_usage \u001b[33mSKIPPED\u001b[0m (rlimit su...)\n",
      "\n",
      "\u001b[32m======================== \u001b[32m\u001b[1m23 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m\u001b[32m in 3.71s\u001b[0m\u001b[32m ========================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest tests/test_tokenizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cde36d",
   "metadata": {},
   "source": [
    "### Problem (tokenizer_experiments): Experiments with tokenizers (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f0e4f",
   "metadata": {},
   "source": [
    "Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30053fff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "270ba725",
   "metadata": {},
   "source": [
    "What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ccffe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f022dfce",
   "metadata": {},
   "source": [
    "Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd9550",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85748033",
   "metadata": {},
   "source": [
    "Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We’ll use this later to train our language model. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is uint16 an appropriate choice?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49b952",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd2d86f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f65a24",
   "metadata": {},
   "source": [
    "## 3. Transformer Language Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b7156",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927015802915263462"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391bfd8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65c49d52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ad770",
   "metadata": {},
   "source": [
    "## 4. Training a Transformer LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2119e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d283b2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4debc8fa",
   "metadata": {},
   "source": [
    "## 5. Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02176d0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25710949",
   "metadata": {},
   "source": [
    "## 6. Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c81cb6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "799684cc",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dec8c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
