{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8260d6f5",
   "metadata": {},
   "source": [
    "## 2. Byte-Pair Encoding (BPE) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11868cba",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927397109025473129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36917f3a",
   "metadata": {},
   "source": [
    "### Problem (unicode1): Understanding Unicode (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b421fef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29275"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('牛')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d9dc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'牛'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(29275)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab79eab4",
   "metadata": {},
   "source": [
    "(a) What Unicode character does chr(0) return?\n",
    "\n",
    "**Deliverable**: A one-sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4736cb3",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">null character</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d903c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87c7c6",
   "metadata": {},
   "source": [
    "(b) How does this character’s string representation (`__repr__()`) differ from its printed representation?\n",
    "\n",
    "**Deliverable**: A one-sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d744a1cc",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "The string representation (`__repr__()`) of the null character explicitly shows its escape sequence `'\\x00'`, whereas its printed representation typically appears as an empty string or nothing at all because it is a non-printable character.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2e55c",
   "metadata": {},
   "source": [
    "(c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations: \n",
    "\n",
    "```python\n",
    ">>> chr(0)\n",
    ">>> print(chr(0))\n",
    ">>> \"this is a test\" + chr(0) + \"string\"\n",
    ">>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "```\n",
    "\n",
    "**Deliverable**: A one-sentence response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096cff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "chr(0)\n",
    "print(chr(0))\n",
    "\"this is a test\" + chr(0) + \"string\"\n",
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88ae17",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "it is embedded within the string but is typically not visible when printed.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644003d",
   "metadata": {},
   "source": [
    "### Problem (unicode2): Unicode Encodings (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3bee5",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings.\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96790864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "<class 'bytes'>\n",
      "UTF-8 value:  104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33\n",
      "b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00'\n",
      "<class 'bytes'>\n",
      "UTF-16 value:  255, 254, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 33, 0, 32, 0, 83, 48, 147, 48, 107, 48, 97, 48, 111, 48, 33, 0\n",
      "b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00S0\\x00\\x00\\x930\\x00\\x00k0\\x00\\x00a0\\x00\\x00o0\\x00\\x00!\\x00\\x00\\x00'\n",
      "<class 'bytes'>\n",
      "UTF-32 value:  255, 254, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 33, 0, 0, 0, 32, 0, 0, 0, 83, 48, 0, 0, 147, 48, 0, 0, 107, 48, 0, 0, 97, 48, 0, 0, 111, 48, 0, 0, 33, 0, 0, 0\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(type(utf8_encoded))\n",
    "print(\"UTF-8 value: \", \", \".join(map(str, list(utf8_encoded))))\n",
    "utf16_encoded = test_string.encode(\"utf-16\")\n",
    "print(utf16_encoded)\n",
    "print(type(utf16_encoded))\n",
    "print(\"UTF-16 value: \", \", \".join(map(str, list(utf16_encoded))))\n",
    "utf32_encoded = test_string.encode(\"utf-32\")\n",
    "print(utf32_encoded)\n",
    "print(type(utf32_encoded))\n",
    "print(\"UTF-32 value: \", \", \".join(map(str, list(utf32_encoded))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b4e5a",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "UTF-8 is the most popular way and often more space-efficient\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6079a5d",
   "metadata": {},
   "source": [
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into\n",
    "a Unicode string. Why is this function incorrect? Provide an example of an input byte string\n",
    "that yields incorrect results.\n",
    "``` python\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "'hello'\n",
    "```\n",
    "\n",
    "**Deliverable**: An example input byte string for which decode_utf8_bytes_to_str_wrong pro-\n",
    "duces incorrect output, with a one-sentence explanation of why the function is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e394ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n\u001b[32m      3\u001b[39m test_string = \u001b[33m\"\u001b[39m\u001b[33mhello! こんにちは!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_string\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbytestring\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "test_string = \"hello! こんにちは!\"\n",
    "decode_utf8_bytes_to_str_wrong(test_string.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896e2f2",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "The function incorrectly attempts to decode each byte individually, leading to a UnicodeDecodeError because multi-byte UTF-8 characters cannot be decoded one byte at a time.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1db7bd",
   "metadata": {},
   "source": [
    "(c) Give a two byte sequence that does not decode to any Unicode character(s).\n",
    "\n",
    "**Deliverable**: An example, with a one-sentence explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3ff18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to decode: b'\\xc2\\x00'\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc2 in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m invalid_bytes = \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\xc2\u001b[39;00m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to decode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_bytes\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m decoded_string = \u001b[43minvalid_bytes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m decoded_string\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xc2 in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "invalid_bytes = b'\\xc2\\x00'\n",
    "print(f\"Attempting to decode: {invalid_bytes!r}\")\n",
    "decoded_string = invalid_bytes.decode(\"utf-8\")\n",
    "decoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2881a",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "b'\\xc2\\x00'\n",
    "because while 0xc2 is a valid start byte for a two-byte UTF-8 sequence, 0x00 is not a valid continuation byte.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05b077bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "# requires `regex` package\n",
    "import regex as re\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f2997",
   "metadata": {},
   "source": [
    "### Problem (train_bpe): BPE Tokenizer Training (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d758a",
   "metadata": {},
   "source": [
    "**Deliverable**: Write a function that, given a path to an input text file, trains a (byte-level) BPE\n",
    "tokenizer. Your BPE training function should handle (at least) the following input parameters:\n",
    "\n",
    "`input_path`: str Path to a text file with BPE tokenizer training data.\n",
    "\n",
    "`vocab_size`: int A positive integer that defines the maximum final vocabulary size (including the\n",
    "initial byte vocabulary, vocabulary items produced from merging, and any special tokens).\n",
    "\n",
    "`special_tokens`: list[str] A list of strings to add to the vocabulary. These special tokens do not\n",
    "otherwise affect BPE training.\n",
    "\n",
    "Your BPE training function should return the resulting vocabulary and merges:\n",
    "\n",
    "`vocab`: dict[int, bytes] The tokenizer vocabulary, a mapping from int (token ID in the vocabu-\n",
    "lary) to bytes (token bytes).\n",
    "\n",
    "`merges`: list[tuple[bytes, bytes]] A list of BPE merges produced from training. Each list item\n",
    "is a tuple of bytes (<token1>, <token2>), representing that <token1> was merged with\n",
    "<token2>. The merges should be ordered by order of creation.\n",
    "\n",
    "To test your BPE training function against our provided tests, you will first need to implement the\n",
    "test adapter at `[adapters.run_train_bpe]`. Then, run `uv run pytest tests/test_train_bpe.py`.\n",
    "Your implementation should be able to pass all tests. Optionally (this could be a large time-investment),\n",
    "you can implement the key parts of your training method using some systems language, for instance\n",
    "`C++` (consider `cppyy` for this) or `Rust` (using `PyO3`). If you do this, be aware of which operations\n",
    "require copying vs reading directly from Python memory, and make sure to leave build instructions, or\n",
    "make sure it builds using only `pyproject.toml`. Also note that the GPT-2 regex is not well-supported\n",
    "in most regex engines and will be too slow in most that do. We have verified that Oniguruma is\n",
    "reasonably fast and supports negative lookahead, but the `regex` package in Python is, if anything,\n",
    "even faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96961ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 3 items\n",
      "\n",
      "tests/test_train_bpe.py::test_train_bpe_speed \u001b[32mPASSED\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe \u001b[32mPASSED\u001b[0m\n",
      "tests/test_train_bpe.py::test_train_bpe_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 2.95s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest tests/test_train_bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d95ab",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414daf3",
   "metadata": {},
   "source": [
    "Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. Make sure to add the TinyStories `<|endoftext|>` special token to the vocabulary.\n",
    "Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "Resource requirements: ≤30 minutes (no GPUs), ≤ 30GB RAM\n",
    "\n",
    "**Hint** You should be able to get under 2 minutes for BPE training using multiprocessing during\n",
    "pretokenization and the following two facts:\n",
    "\n",
    "(a) The `<|endoftext|>` token delimits documents in the data files.\n",
    "\n",
    "(b) The `<|endoftext|>` token is handled as a special case before the BPE merges are applied.\n",
    "\n",
    "**Deliverable:** A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e7315",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "The tokenizer was trained in approximately 3 minutes (189.14 seconds) with a peak memory usage of 136.51 MB, both well within the resource limits. The longest token, a 512-byte sequence of the repeating character 's'\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712493d9",
   "metadata": {},
   "source": [
    "Profile your code. What part of the tokenizer training process takes the most time?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec58ce",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "the pre-tokenization step.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd7318",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc079de",
   "metadata": {},
   "source": [
    "Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary\n",
    "size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What\n",
    "is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "**Resource requirements**: ≤12 hours (no GPUs), ≤ 100GB RAM\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4a91d",
   "metadata": {},
   "source": [
    "<span style=\"background-color: yellow; color: black\">\n",
    "Because it takes too long for my computer to run this, I directly used the tokenizer library of Hugging Face to generate the file\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b47c18",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "The longest token is '--------------------------------'\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638596ed",
   "metadata": {},
   "source": [
    "Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049251e",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "TinyStories' tokenizer mainly learns simple narrative vocabulary, while OpenWebText's tokenizer contains more structural and formatted characters derived from network text.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31467b10",
   "metadata": {},
   "source": [
    "### Problem (tokenizer): Implementing the tokenizer (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4723663",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement a Tokenizer class that, given a vocabulary and a list of merges, encodes\n",
    "text into integer IDs and decodes integer IDs into text. Your tokenizer should also support user-provided\n",
    "special tokens (appending them to the vocabulary if they aren’t already there). We recommend the\n",
    "following interface:\n",
    "\n",
    "`def __init__(self, vocab, merges, special_tokens=None)` Construct a tokenizer from a given vocabulary, list of merges, and (optionally) a list of special tokens. This function should accept the following parameters:\n",
    "\n",
    "``` python\n",
    "vocab: dict[int, bytes]\n",
    "merges: list[tuple[bytes, bytes]]\n",
    "special_tokens: list[str] | None = None\n",
    "```\n",
    "\n",
    "`def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None)` Class method that constructs and return a Tokenizer from a serialized vocabulary and list of merges (in the same format that your BPE training code output) and (optionally) a list of special tokens. This method should accept the following additional parameters:\n",
    "\n",
    "``` python\n",
    "vocab_filepath: str\n",
    "merges_filepath: str\n",
    "special_tokens: list[str] | None = None\n",
    "```\n",
    "\n",
    "`def encode(self, text: str) -> list[int]` Encode an input text into a sequence of token IDs.\n",
    "\n",
    "`def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]` Given an iterable of strings (e.g., a Python file handle), return a generator that lazily yields token IDs. This is required for memory-eﬀicient tokenization of large files that we cannot directly load into memory.\n",
    "\n",
    "`def decode(self, ids: list[int]) -> str` Decode a sequence of token IDs into text.\n",
    "\n",
    "To test your Tokenizer against our provided tests, you will first need to implement the test adapter at `[adapters.get_tokenizer]`. Then, run `uv run pytest tests/test_tokenizer.py`. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de824fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 25 items\n",
      "\n",
      "tests/test_tokenizer.py::test_roundtrip_empty \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_empty_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_single_character \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_single_character_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_single_unicode_character \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_ascii_string \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_ascii_string_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_unicode_string \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_unicode_string_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_overlapping_special_tokens \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_address_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_address_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_german_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_german_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_tinystories_sample_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_tinystories_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_special_token_trailing_newlines \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_special_token_double_newline_non_whitespace \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken \u001b[32mPASSED\u001b[0m\n",
      "tests/test_tokenizer.py::test_encode_iterable_memory_usage \u001b[33mSKIPPED\u001b[0m (...)\n",
      "tests/test_tokenizer.py::test_encode_memory_usage \u001b[33mSKIPPED\u001b[0m (rlimit su...)\n",
      "\n",
      "\u001b[32m======================== \u001b[32m\u001b[1m23 passed\u001b[0m, \u001b[33m2 skipped\u001b[0m\u001b[32m in 3.30s\u001b[0m\u001b[32m ========================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest tests/test_tokenizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cde36d",
   "metadata": {},
   "source": [
    "### Problem (tokenizer_experiments): Experiments with tokenizers (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f0e4f",
   "metadata": {},
   "source": [
    "Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30053fff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "270ba725",
   "metadata": {},
   "source": [
    "What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ccffe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f022dfce",
   "metadata": {},
   "source": [
    "Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd9550",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85748033",
   "metadata": {},
   "source": [
    "Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We’ll use this later to train our language \\text{model}. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is uint16 an appropriate choice?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49b952",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd2d86f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f65a24",
   "metadata": {},
   "source": [
    "## 3. Transformer Language Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b7156",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927015802915263462\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "  <img src=\"data/transformer.png\" alt=\"Transformer Image\" style=\"width: 30%;\">\n",
    "  <img src=\"data/prenorm_transformer.png\" alt=\"Pre-Norm Transformer Image\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24826371",
   "metadata": {},
   "source": [
    "### Problem (linear): Implementing the linear module (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21033471",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement a `Linear` class that inherits from `torch.nn.Module` and performs a linear transformation. Your implementation should follow the interface of PyTorch’s built-in `nn.Linear` module, except for not having a bias argument or parameter. We recommend the following interface:\n",
    "\n",
    "`def __init__(self, in_features, out_features, device=None, dtype=None)` Construct a\n",
    "linear transformation module. This function should accept the following parameters:\n",
    "``` python\n",
    "in_features: int final dimension of the input\n",
    "out_features: int final dimension of the output\n",
    "device: torch.device | None = None Device to store the parameters on\n",
    "dtype: torch.dtype | None = None Data type of the parameters\n",
    "```\n",
    "\n",
    "`def forward(self, x: torch.Tensor) -> torch.Tensor` Apply the linear transformation to the input.\n",
    "\n",
    "Make sure to:\n",
    "\n",
    "- subclass `nn.Module`\n",
    "- call the superclass constructor\n",
    "- construct and store your parameter as $W$ (not $W^T$) for memory ordering reasons, putting it in an `nn.Parameter`\n",
    "- of course, don’t use `nn.Linear` or `nn.functional.linear`\n",
    "\n",
    "For initializations, use the settings from above along with `torch.nn.init.trunc_normal_` to\n",
    "initialize the weights.\n",
    "To test your Linear module, implement the test adapter at `[adapters.run_linear]`. The adapter\n",
    "should load the given weights into your Linear module. You can use `Module.load_state_dict` for\n",
    "this purpose. Then, run `uv run pytest -k test_linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb7c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_linear \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.55s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb0415",
   "metadata": {},
   "source": [
    "### Problem (embedding): Implement the embedding module (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5f11d",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement the `Embedding` class that inherits from `torch.nn.Module` and performs an\n",
    "embedding lookup. Your implementation should follow the interface of PyTorch’s built-in\n",
    "`nn.Embedding` module. We recommend the following interface:\n",
    "\n",
    "`def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None)` Construct an embedding module. This function should accept the following parameters:\n",
    "\n",
    "``` python\n",
    "num_embeddings: int Size of the vocabulary\n",
    "embedding_dim: int Dimension of the embedding vectors, i.e., d_model\n",
    "device: torch.device | None = None Device to store the parameters on\n",
    "dtype: torch.dtype | None = None Data type of the parameters\n",
    "```\n",
    "\n",
    "`def forward(self, token_ids: torch.Tensor) -> torch.Tensor` Lookup the embedding vectors for the given token IDs.\n",
    "\n",
    "Make sure to:\n",
    "- subclass nn.Module\n",
    "- call the superclass constructor\n",
    "- initialize your embedding matrix as a nn.Parameter\n",
    "- store the embedding matrix with the d_model being the final dimension\n",
    "- of course, don’t use nn.Embedding or nn.functional.embedding\n",
    "\n",
    "Again, use the settings from above for initialization, and use `torch.nn.init.trunc_normal_` to\n",
    "initialize the weights.\n",
    "\n",
    "To test your implementation, implement the test adapter at `[adapters.run_embedding]`. Then, run\n",
    "`uv run pytest -k test_embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f2866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_embedding \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.13s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81feed",
   "metadata": {},
   "source": [
    "### Problem (rmsnorm): Root Mean Square Layer Normalization (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887c53b",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement `RMSNorm` as a `torch.nn.Module`. We recommend the following interface:\n",
    "\n",
    "`def __init__(self, d_model: int, eps: float = 1e-5, device=None, dtype=None)` Construct the RMSNorm module. This function should accept the following parameters:\n",
    "\n",
    "``` python\n",
    "d_model: int Hidden dimension of the model\n",
    "eps: float = 1e-5 Epsilon value for numerical stability\n",
    "device: torch.device | None = None Device to store the parameters on\n",
    "dtype: torch.dtype | None = None Data type of the parameters\n",
    "```\n",
    "\n",
    "`def forward(self, x: torch.Tensor) -> torch.Tensor` Process an input tensor of shape (`batch_size`, `sequence_length`, `d_model`) and return a tensor of the same shape.\n",
    "\n",
    "**Note**: Remember to upcast your input to `torch.float32` before performing the normalization (and\n",
    "later downcast to the original `dtype`), as described above.\n",
    "\n",
    "To test your implementation, implement the test adapter at `[adapters.run_rmsnorm]`. Then, run `uv\n",
    "run pytest -k test_rmsnorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007bc701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_rmsnorm \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_rmsnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df4b20",
   "metadata": {},
   "source": [
    "### Problem (positionwise_feedforward): Implement the position-wise feed-forward network (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c92b55",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement the `SwiGLU` feed-forward network, composed of a `SiLU` activation function and a `GLU`.\n",
    "\n",
    "**Note**: in this particular case, you should feel free to use torch.sigmoid in your implementation for numerical stability.\n",
    "\n",
    "You should set dff to approximately $\\frac{8}{3}$ × $d_\\text{model}$ in your implementation, while ensuring that the dimensionality of the inner feed-forward layer is a multiple of 64 to make good use of your hardware. To test your implementation against our provided tests, you will need to implement\n",
    "the test adapter at `[adapters.run_swiglu]`. Then, run `uv run pytest -k test_swiglu` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c096783b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_swiglu \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.54s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_swiglu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c7b19",
   "metadata": {},
   "source": [
    "### Problem (rope): Implement RoPE (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca76b4e",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement a class `RotaryPositionalEmbedding` that applies RoPE to the input tensor.\n",
    "\n",
    "The following interface is recommended:\n",
    "\n",
    "`def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None)` Construct the\n",
    "RoPE module and create buffers if needed.\n",
    "\n",
    "``` python\n",
    "theta: float Θ value for the RoPE\n",
    "d_k: int dimension of query and key vectors\n",
    "max_seq_len: int Maximum sequence length that will be inputted\n",
    "device: torch.device | None = None Device to store the buffer on\n",
    "```\n",
    "\n",
    "`def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor` Process an input tensor of shape `(..., seq_len, d_k)` and return a tensor of the same shape. Note that you should tolerate x with an arbitrary number of batch dimensions. You should assume that the token positions are a tensor of shape `(..., seq_len)` specifying the token positions of `x` along the sequence dimension.\n",
    "\n",
    "You should use the token positions to slice your (possibly precomputed) cos and sin tensors along the sequence dimension.\n",
    "\n",
    "To test your implementation, complete [adapters.run_rope] and make sure it passes `uv run pytest -k test_rope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a43ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_rope \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.14s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_rope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce930c",
   "metadata": {},
   "source": [
    "### Problem (softmax): Implement softmax (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe33778",
   "metadata": {},
   "source": [
    "**Deliverable**: Write a function to apply the `softmax` operation on a tensor. Your function should take two parameters: a tensor and a dimension $i$, and apply softmax to the $i$-th dimension of the input tensor. The output tensor should have the same shape as the input tensor, but its $i$-th dimension will now have a normalized probability distribution. Use the trick of subtracting the maximum value in the $i$-th dimension from all elements of the $i$-th dimension to avoid numerical stability issues.\n",
    "\n",
    "To test your implementation, complete `[adapters.run_softmax]` and make sure it passes `uv run pytest -k test_softmax_matches_pytorch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae1a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_nn_utils.py::test_softmax_matches_pytorch \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_softmax_matches_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74401631",
   "metadata": {},
   "source": [
    "### Problem (scaled_dot_product_attention): Implement scaled dot-product attention (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe00ff",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement the scaled dot-product attention function. Your implementation should handle keys and queries of shape `(batch_size, ..., seq_len, d_k)` and values of shape `(batch_size, ..., seq_len, d_v)`, where `...` represents any number of other batch-like dimensions (if provided). The implementation should return an output with the shape `(batch_size, ..., d_v)`. See section $3.3$ for a discussion on batch-like dimensions.\n",
    "\n",
    "Your implementation should also support an optional user-provided boolean mask of shape `(seq_len, seq_len)`. The attention probabilities of positions with a mask value of `True` should collectively sum\n",
    "to $1$, and the attention probabilities of positions with a mask value of `False` should be zero.\n",
    "To test your implementation against our provided tests, you will need to implement the test adapter\n",
    "at `[adapters.run_scaled_dot_product_attention]`.\n",
    "\n",
    "`uv run pytest -k test_scaled_dot_product_attention` tests your implementation on third-order input tensors, while `uv run pytest -k test_4d_scaled_dot_product_attention` tests your implementation on fourth-order input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c20af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_scaled_dot_product_attention \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.49s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248483ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_4d_scaled_dot_product_attention \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.58s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_4d_scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e58f1b",
   "metadata": {},
   "source": [
    "### Problem (multihead_self_attention): Implement causal multi-head self-attention (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233c4ea",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement causal multi-head self-attention as a `torch.nn.Module`. Your implementation should accept (at least) the following parameters:\n",
    "\n",
    "``` python\n",
    "d_model: int Dimensionality of the Transformer block inputs.\n",
    "num_heads: int Number of heads to use in multi-head self-attention.\n",
    "```\n",
    "\n",
    "Folllowing $\\text{Vaswani et al. [2017]}$, set $d_k=d_v=\\frac{d_{model}}h$. To test your implementation against our provided tests, implement the test adapter at `[adapters.run_multihead_self_attention]`. Then, run `uv run pytest -k test_multihead_self_attention` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf6a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 46 deselected / 2 selected\n",
      "\n",
      "tests/test_model.py::test_multihead_self_attention \u001b[32mPASSED\u001b[0m\n",
      "tests/test_model.py::test_multihead_self_attention_with_rope \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m46 deselected\u001b[0m\u001b[32m in 1.51s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_multihead_self_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414dba8",
   "metadata": {},
   "source": [
    "### Problem (transformer_block): Implement the Transformer block (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8755a5",
   "metadata": {},
   "source": [
    "Implement the pre-norm Transformer block as described in $§3.5$ and illustrated in $\\text{Figure} 2$. Your\n",
    "Transformer block should accept (at least) the following parameters.\n",
    "\n",
    "``` python\n",
    "d_model: int Dimensionality of the Transformer block inputs.\n",
    "num_heads: int Number of heads to use in multi-head self-attention.\n",
    "d_ff: int Dimensionality of the position-wise feed-forward inner layer.\n",
    "```\n",
    "\n",
    "To test your implementation, implement the adapter `[adapters.run_transformer_block]`. Then run `uv run pytest -k test_transformer_block` to test your implementation.\n",
    "\n",
    "**Deliverable**: Transformer block code that passes the provided tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0611eb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_model.py::test_transformer_block \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.53s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_transformer_block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888d755",
   "metadata": {},
   "source": [
    "### Problem (transformer_lm): Implementing the Transformer LM (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fb709",
   "metadata": {},
   "source": [
    "Time to put it all together! Implement the Transformer language model as described in $§3.1$ and illustrated in $\\text{Figure} 1$. At minimum, your implementation should accept all the aforementioned construction parameters for the Transformer block, as well as these additional parameters:\n",
    "\n",
    "``` python\n",
    "vocab_size: int The size of the vocabulary, necessary for determining the dimensionality of the token embedding matrix.\n",
    "context_length: int The maximum context length, necessary for determining the dimensionality of the position embedding matrix.\n",
    "num_layers: int The number of Transformer blocks to use.\n",
    "```\n",
    "\n",
    "To test your implementation against our provided tests, you will first need to implement the test adapter at `[adapters.run_transformer_lm]`. Then, run `uv run pytest -k test_transformer_lm` to test your implementation.\n",
    "\n",
    "**Deliverable**: A Transformer LM module that passes the above tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f5a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 46 deselected / 2 selected\n",
      "\n",
      "tests/test_model.py::test_transformer_lm \u001b[32mPASSED\u001b[0m\n",
      "tests/test_model.py::test_transformer_lm_truncated_input \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m46 deselected\u001b[0m\u001b[32m in 1.56s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_transformer_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89a0c7",
   "metadata": {},
   "source": [
    "### Problem (transformer_accounting): Transformer LM resource accounting (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5545886c",
   "metadata": {},
   "source": [
    "Consider GPT-2 XL, which has the following configuration:\n",
    "\n",
    "``` python\n",
    "vocab_size : 50,257\n",
    "context_length : 1,024\n",
    "num_layers : 48\n",
    "d_model : 1,600\n",
    "num_heads : 25\n",
    "d_ff : 6,400\n",
    "```\n",
    "\n",
    "Suppose we constructed our model using this configuration. How many trainable parameters\n",
    "would our model have? Assuming each parameter is represented using single-precision floating\n",
    "point, how much memory is required to just load this model?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "``` markdown\n",
    "**token_embeddings**: vocab_size * d_model = 50,257 * 1,600 = 80,411,200\n",
    "**note**: embedding is is a **lookup**, but it is mathematically equivalent to a **matrix multiplication** with a **one-hot encoded vector**.\n",
    "\n",
    "**layers**:\n",
    "- **attn**:\n",
    "  - q_proj: d_model × d_model = 1,600 × 1,600 = 2,560,000\n",
    "  - k_proj: d_model × d_model = 1,600 × 1,600 = 2,560,000\n",
    "  - v_proj: d_model × d_model = 1,600 × 1,600 = 2,560,000\n",
    "  - output_proj: d_model × d_model = 1,600 × 1,600 = 2,560,000\n",
    "- **ffn**:\n",
    "  - w1: d_model × d_ff = 1,600 × 6,400 = 10,240,000\n",
    "  - w2: d_ff × d_model = 6,400 × 1,600 = 10,240,000\n",
    "  - w3: d_model × d_ff = 1,600 × 6,400 = 10,240,000\n",
    "  **ln1**: d_model = 1600\n",
    "  **ln2**: d_model = 1600\n",
    "- 40,963,200 x num_layers = 40,963,200 x 48 = 1,966,233,600\n",
    "\n",
    "**ln_final**：d_model = 1,600\n",
    "\n",
    "**lm_head**: vocab_size * d_model = 50,257 * 1,600 = 80,411,200\n",
    "```\n",
    "\n",
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "2,127,058,600 parameters, × 4 bytes = 7.92 GB\n",
    "</span>\n",
    "\n",
    "---\n",
    "\n",
    "Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped\n",
    "model. How many `FLOPs` do these matrix multiplies require in total? Assume that our input\n",
    "sequence has context_length tokens.\n",
    "\n",
    "**Deliverable**: A list of matrix multiplies (with descriptions), and the total number of `FLOPs`\n",
    "required.\n",
    "\n",
    "---\n",
    "\n",
    "``` markdown\n",
    "**Transformer Block**\n",
    "- MHA\n",
    "- - q_proj: (1024, 1600) × (1600, 1600) → FLOPs: 2 × 1024 × 1600 × 1600 = 5,242,880,000\n",
    "- - k_proj: (1024, 1600) × (1600, 1600) → FLOPs: 2 × 1024 × 1600 × 1600 = 5,242,880,000\n",
    "- - v_proj: (1024, 1600) × (1600, 1600) → FLOPs: 2 × 1024 × 1600 × 1600 = 5,242,880,000\n",
    "- - attention scores: (25, 1024, 64) × (25, 64, 1024) → FLOPs: 2 × 25 × 1024 × 64 × 1024 = 3,355,443,200\n",
    "- - attention: (25, 1024, 1024) × (25, 1024, 64) → FLOPs: 2 × 25 × 1024 × 1024 × 64 = 3,355,443,200\n",
    "- - output_proj: (1024, 1600) × (1600, 1600) → FLOPs: 2 × 1024 × 1600 × 1600 = 5,242,880,000\n",
    "- FFN\n",
    "- - w1: (1024, 1600) × (1600, 6400) → FLOPs: 2 × 1024 × 1600 × 6400 = 20,971,520,000\n",
    "- - w3: (1024, 1600) × (1600, 6400) → FLOPs: 2 × 1024 × 1600 × 6400 = 20,971,520,000\n",
    "- - w2: (1024, 6400) × (6400, 1600) → FLOPs: 2 × 1024 × 6400 × 1600 = 20,971,520,000\n",
    "- 90,596,966,400 * 48 = 4,348,654,387,200\n",
    "\n",
    "**LM Head**: (1024, 1600) × (1600, 50257) → FLOPs: 2 × 1024 × 1600 × 50257 = 164,681,932,800\n",
    "```\n",
    "\n",
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "4,513,297,920,000 FLOPs\n",
    "</span>\n",
    "\n",
    "---\n",
    "\n",
    "Based on your analysis above, which parts of the model require the most `FLOPs`?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "FFN, as it projects the hidden state from d_model to a larger intermediate dimension d_ff (4x in this case), and then projects it back, which involves matrix multiplication operations much larger than the self attention part.\n",
    "</span>\n",
    "\n",
    "---\n",
    "\n",
    "Repeat your analysis with GPT-2 small (12 layers, 768 d_model, 12 heads), GPT-2 medium (24\n",
    "layers, 1024 d_model, 16 heads), and GPT-2 large (36 layers, 1280 d_model, 20 heads). As the\n",
    "model size increases, which parts of the Transformer LM take up proportionally more or less of\n",
    "the total `FLOPs`?\n",
    "\n",
    "**Deliverable**: For each model, provide a breakdown of model components and its associated\n",
    "`FLOPs` (as a proportion of the total `FLOPs` required for a forward pass). In addition, provide a\n",
    "one-to-two sentence description of how varying the model size changes the proportional `FLOPs`\n",
    "of each component.\n",
    "\n",
    "---\n",
    "\n",
    "| model            | d_model (d) | FFN FLOPs         | Attention FLOPs | FFN %  | Attention % |\n",
    "| ---------------- | ----------- | ----------------- | --------------- | ------ | ----------- |\n",
    "| **GPT-2 small**  | 768         | 11,509,596,160    | 4,731,174,912   | ~64.3% | ~35.7%      |\n",
    "| **GPT-2 medium** | 1024        | 25,769,803,776    | 12,884,901,888  | ~66.7% | ~33.3%      |\n",
    "| **GPT-2 large**  | 1280        | 40,265,318,400    | 18,790,481,920  | ~68.2% | ~31.8%      |\n",
    "| **GPT-2 XL**     | 1600        | 62,914,560,000    | 27,682,406,400  | ~69.4% | ~30.6%      |\n",
    "\n",
    "\n",
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "The proportion of FFN in total FLOPs will relatively increase, while the proportion of self attention will relatively decrease\n",
    "</span>\n",
    "\n",
    "---\n",
    "\n",
    "Take GPT-2 XL and increase the context length to 16,384. How does the total `FLOPs` for one\n",
    "forward pass change? How do the relative contribution of `FLOPs` of the model components\n",
    "change?\n",
    "\n",
    "**Deliverable**: A one-to-two sentence response.\n",
    "\n",
    "---\n",
    "\n",
    "``` markdown\n",
    "**Attention** increase sequence length dimension by 16 times, FLOPs increase by 16 times\n",
    "**FFN**: Increase sequence length dimension by 16 times, FLOPs increase by 16 times\n",
    "**Total**L FLOPs increase by approximately 16 times\n",
    "\n",
    "**FFN** still dominates, but the proportion of sequence length related calculations (Q × K and attention × V) for attention will slightly increase, as the complexity of these operations is O(n^2), while other operations are O(n).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c49d52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ad770",
   "metadata": {},
   "source": [
    "## 4. Training a Transformer LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac309850",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927132746578366957"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2119e",
   "metadata": {},
   "source": [
    "### Problem (cross_entropy): Implement Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d283b2c",
   "metadata": {},
   "source": [
    "**Deliverable**: Write a function to compute the cross entropy loss, which takes in predicted logits $(o_i)$ and targets $(x_{i+1})$ and computes the cross entropy $ℓi = − \\text{log} (\\text{softmax}(o_i)[x_{i+1}])$. Your function should handle the following:\n",
    "\n",
    "- Subtract the largest element for numerical stability.\n",
    "- Cancel out log and exp whenever possible.\n",
    "- Handle any additional batch dimensions and return the $\\text{average}$ across the batch. As with section $3.3$, we assume batch-like dimensions always come first, before the vocabulary size dimension.\n",
    "\n",
    "Implement `[adapters.run_cross_entropy]`, then run `uv run pytest -k test_cross_entropy` to test your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dab7c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_nn_utils.py::test_cross_entropy \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a0245",
   "metadata": {},
   "source": [
    "### Problem (learning_rate_tuning): Tuning the learning rate (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c0885b",
   "metadata": {},
   "source": [
    "As we will see, one of the hyperparameters that affects training the most is the learning rate. Let’s see that in practice in our toy example. Run the `SGD` example above with three other values for the learning rate: `1e1`, `1e2`, and `1e3`, for just 10 training iterations. What happens with the loss for each of these learning rates? Does it decay faster, slower, or does it diverge (i.e., increase over the course of training)?\n",
    "\n",
    "**Deliverable**: A one-two sentence response with the behaviors you observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61209b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "from typing import Optional\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # Get the learning rate.\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "            state = self.state[p] # Get state associated with p.\n",
    "            t = state.get(\"t\", 0) # Get iteration number from the state, or initial value.\n",
    "            grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "            p.data -= lr / math.sqrt(t + 1) * grad # Update weight tensor in-place.\n",
    "            state[\"t\"] = t + 1 # Increment iteration number.\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92230857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr):\n",
    "    weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "    opt = SGD([weights], lr=lr)\n",
    "\n",
    "    for t in range(10):\n",
    "        opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "        loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "        print(loss.cpu().item())\n",
    "        loss.backward() # Run backward pass, which computes gradients.\n",
    "        opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333a081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.122913360595703\n",
      "14.15866470336914\n",
      "10.437161445617676\n",
      "8.165966033935547\n",
      "6.614432334899902\n",
      "5.484122276306152\n",
      "4.625129222869873\n",
      "3.952305316925049\n",
      "3.413126230239868\n",
      "2.973212242126465\n"
     ]
    }
   ],
   "source": [
    "train(1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e931da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.372173309326172\n",
      "29.372173309326172\n",
      "5.039466857910156\n",
      "0.12060579657554626\n",
      "2.271793980606257e-16\n",
      "2.5320552761258588e-18\n",
      "8.526320552724894e-20\n",
      "5.0791882485921435e-21\n",
      "4.357255665721727e-22\n",
      "4.841394938711859e-23\n"
     ]
    }
   ],
   "source": [
    "train(1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65030da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.06910705566406\n",
      "11576.94921875\n",
      "1999521.125\n",
      "222425376.0\n",
      "18016452608.0\n",
      "1137044619264.0\n",
      "58372153933824.0\n",
      "2511419849310208.0\n",
      "9.25655732500562e+16\n",
      "2.9723837255238287e+18\n"
     ]
    }
   ],
   "source": [
    "train(1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d619b48",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #29B6F6; color: black\">\n",
    "a learning rate of 1e1, the loss decays slowly, while a learning rate of 1e2 causes the loss to decay very rapidly. A learning rate of 1e3 is too high and causes the loss to diverge, increasing exponentially throughout training.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f72d2b6",
   "metadata": {},
   "source": [
    "### Problem (adamw): Implement AdamW (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb86075",
   "metadata": {},
   "source": [
    "![](./data/AdamW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479651a",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement the AdamW optimizer as a subclass of `torch.optim.Optimizer`. Your class should take the learning rate $\\alpha$ in `__init__`, as well as the $\\beta$, $\\epsilon$ and $\\lambda$ hyperparameters. To help you keep state, the base Optimizer class gives you a dictionary `self.state`, which maps `nn.Parameter` objects to a dictionary that stores any information you need for that parameter (for AdamW, this would be the moment estimates). Implement `[adapters.get_adamw_cls]` and make sure it passes `uv run pytest -k test_adamw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ce36c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_optimizer.py::test_adamw \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 2.30s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_adamw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563024aa",
   "metadata": {},
   "source": [
    "### Problem (adamwAccounting): Resource accounting for training with AdamW (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca111d9",
   "metadata": {},
   "source": [
    "Let us compute how much memory and compute running AdamW requires. Assume we are using `float32` for every tensor.\n",
    "\n",
    "How much peak memory does running `AdamW` require? Decompose your answer based on the memory usage of the parameters, activations, gradients, and optimizer state. Express your answer in terms of the batch_size and the model hyperparameters (`vocab_size`, `context_length`, `num_layers`, `d_model`, `num_heads`). Assume `d_ff = 4 × d_model`.\n",
    "\n",
    "For simplicity, when calculating memory usage of activations, consider only the following components:\n",
    "- Transformer block\n",
    "  - RMSNorm(s)\n",
    "  - Multi-head self-attention sublayer: $QKV$ projections, $Q^TK$ matrix multiply, softmax, weighted sum of values, output projection.\n",
    "  - Position-wise feed-forward: $W_1$ matrix multiply, $\\text{SiLU}$, $W_2$ matrix multiply\n",
    "- final RMSNorm\n",
    "- output embedding\n",
    "- cross-entropy on logits\n",
    "\n",
    "**Deliverable**: An algebraic expression for each of parameters, activations, gradients, and optimizer state, as well as the total.\n",
    "\n",
    "---\n",
    "* **Embedding:**\n",
    "\n",
    "  * `token_embeddings`: `vocab_size × d_model`\n",
    "  * `lm_head`: `d_model × vocab_size`\n",
    "\n",
    "* **Per Transformer Block:**\n",
    "\n",
    "  * Attention: `4 × d_model²`\n",
    "  * Feedforward (SwiGLU): `3 × d_model × d_ff`\n",
    "  * LayerNorms: `2 × d_model`\n",
    "  * Total per block: `4d_model² + 3d_model × d_ff + 2d_model`\n",
    "\n",
    "* **Final LayerNorm:** `d_model`\n",
    "\n",
    "* **Total Parameters:**\n",
    "  Given `d_ff = 4 × d_model`,\n",
    "\n",
    "  ```\n",
    "  P = 2 × vocab_size × d_model  \n",
    "      + num_layers × (16 × d_model² + 2 × d_model)  \n",
    "      + d_model\n",
    "  ```\n",
    "\n",
    "* **Parameter Memory (float32):**\n",
    "  `4 × P` bytes\n",
    "\n",
    "**Activation Memory (Peak)**\n",
    "\n",
    "* **Within a Transformer Block:**\n",
    "\n",
    "  * Inputs/outputs: `batch_size × context_length × d_model`\n",
    "  * **Attention:**\n",
    "\n",
    "    * `Q, K, V`: `3 × batch_size × context_length × d_model`\n",
    "    * `QKᵀ` (attention scores): `batch_size × num_heads × context_length²` <- *dominant*\n",
    "  * **Feedforward (SwiGLU):**\n",
    "\n",
    "    * Peak: `2 × batch_size × context_length × d_ff = 8 × batch_size × context_length × d_model`\n",
    "\n",
    "* **Final Output:**\n",
    "\n",
    "  * `lm_head` logits: `batch_size × context_length × vocab_size`\n",
    "  * Cross-entropy reuses logits\n",
    "\n",
    "* **Peak Activation Memory:**\n",
    "  `4 × (batch_size × context_length × vocab_size)` bytes\n",
    "\n",
    "**Gradient Memory**\n",
    "\n",
    "* Same shape as parameters\n",
    "  → `4 × P` bytes\n",
    "\n",
    "**Optimizer State (AdamW)**\n",
    "\n",
    "* Two float32 tensors per parameter (`exp_avg`, `exp_avg_sq`)\n",
    "  → `8 × P` bytes\n",
    "\n",
    "**Total Peak Memory**\n",
    "\n",
    "```\n",
    "Total = 4 × P                          // parameters\n",
    "      + 4 × batch_size × context_length × vocab_size   // activations\n",
    "      + 4 × P                          // gradients\n",
    "      + 8 × P                          // optimizer states\n",
    "      = 16 × P + 4 × batch_size × context_length × vocab_size bytes\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "```\n",
    "P = 2 × vocab_size × d_model  \n",
    "  + num_layers × (16 × d_model² + 2 × d_model)  \n",
    "  + d_model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Instantiate your answer for a GPT-2 XL-shaped model to get an expression that only depends on the `batch_size`. What is the maximum batch size you can use and still fit within 80GB memory?\n",
    "\n",
    "**Deliverable**: An expression that looks like $a \\cdot$ `batch_size` $+ b$ for numerical values $a$, $b$, and a number representing the maximum batch size.\n",
    "\n",
    "---\n",
    "\n",
    "``` python\n",
    "vocab_size = 50,257  \n",
    "d_model = 1,600  \n",
    "num_layers = 48  \n",
    "d_ff = 4 × d_model = 6,400  \n",
    "context_length = 1,024\n",
    "```\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P &= 2 \\cdot 50257 \\cdot 1600 + 48 \\cdot (16 \\cdot 1600^2 + 2 \\cdot 1600) + 1600 \\\\\n",
    "  &= \\boxed{2,127,057,600} \\text{ parameters}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a &= 4 \\cdot T \\cdot V = 4 \\cdot 1024 \\cdot 50257 = \\boxed{205,852,672} \\\\\n",
    "b &= 16 \\cdot P = 16 \\cdot 2,127,057,600 = \\boxed{34,032,921,600}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$\n",
    "a \\cdot B + b \\leq 85,899,345,920\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{PeakMemory} = 205{,}852{,}672 \\cdot \\text{batch\\_size} + 34{,}032{,}921{,}600}\n",
    "$$\n",
    "\n",
    "max batch size:\n",
    "\n",
    "$$\n",
    "\\boxed{251}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "How many `FLOPs` does running one step of `AdamW` take?\n",
    "\n",
    "**Deliverable**: An algebraic expression, with a brief justification.\n",
    "\n",
    "---\n",
    "Each parameter update involves:\n",
    "  1. **Weight decay:** 2 FLOPs (multiply, subtract)\n",
    "  2. **First moment update:** 4 FLOPs (2 multiplies, 1 subtract, 1 add)\n",
    "  3. **Second moment update:** 5 FLOPs (square, 2 multiplies, subtract, add)\n",
    "  4. **Final update:** 5 FLOPs (sqrt, add, divide, multiply, subtract)\n",
    "\n",
    "* **Total:** 16 FLOPs per parameter (ignoring per-step `lr_t` computation)\n",
    "\n",
    "* For GPT-2 XL (≈2.13B params):\n",
    "\n",
    "  ```\n",
    "  FLOPs = 16 × 2,127,057,600 = 34.03 billion\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "Model `FLOPs` utilization (MFU) is defined as the ratio of observed throughput (tokens per second) relative to the hardware’s theoretical peak FLOP throughput $\\text{[Chowdhery et al., 2022]}$. An NVIDIA A100 GPU has a theoretical peak of `19.5 teraFLOP/s` for `float32` operations. Assuming\n",
    "you are able to get 50% MFU, how long would it take to train a GPT-2 XL for 400K steps and a batch size of 1024 on a single A100? Following $\\text{Kaplan et al. [2020]}$ and $\\text{Hoffmann et al. [2022]}$, assume that the backward pass has twice the `FLOPs` of the forward pass.\n",
    "\n",
    "**Deliverable**: The number of days training would take, with a brief justification.\n",
    "\n",
    "---\n",
    "\n",
    "* **FLOPs per step:**\n",
    "\n",
    "  * Forward: `4.51 × 10¹²`\n",
    "  * Backward: `2 × forward = 9.03 × 10¹²`\n",
    "  * Total per step: `13.54 × 10¹²`\n",
    "\n",
    "* **Total steps:** `400,000`\n",
    "\n",
    "* **Total training FLOPs:**\n",
    "\n",
    "  ```\n",
    "  400,000 × 13.54 × 10¹² = 5.42 × 10¹⁸\n",
    "  ```\n",
    "\n",
    "* **GPU (A100) throughput:**\n",
    "\n",
    "  * Peak: `19.5 × 10¹²` FLOP/s\n",
    "  * Utilization (MFU): 50% → Effective: `9.75 × 10¹²` FLOP/s\n",
    "\n",
    "* **Training time:**\n",
    "\n",
    "  ```\n",
    "  Seconds = 5.42 × 10¹⁸ ÷ 9.75 × 10¹² ≈ 555,483\n",
    "  Days = 555,483 ÷ 86,400 ≈ 6.43\n",
    "  ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315bf02",
   "metadata": {},
   "source": [
    "### Problem (learning_rate_schedule): Implement cosine learning rate schedule with warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ec22b",
   "metadata": {},
   "source": [
    "Write a function that takes $t, \\alpha_\\text{max}, \\alpha_\\text{min}, T_w$ and $T_c$, and returns the learning rate $\\alpha_t$ according to the scheduler defined above. Then implement `[adapters.get_lr_cosine_schedule]` and make sure it passes `uv run pytest -k test_get_lr_cosine_schedule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889b2b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_optimizer.py::test_get_lr_cosine_schedule \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 2.10s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_get_lr_cosine_schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef1905",
   "metadata": {},
   "source": [
    "### Problem (gradient_clipping): Implement gradient clipping (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea168b",
   "metadata": {},
   "source": [
    "Write a function that implements gradient clipping. Your function should take a list of parameters and a maximum $\\ell_2$-norm. It should modify each parameter gradient in place. Use $\\epsilon$ = `10−6` (the PyTorch default). Then, implement the adapter `[adapters.run_gradient_clipping]` and make sure it passes `uv run pytest -k test_gradient_clipping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99222474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_nn_utils.py::test_gradient_clipping \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_gradient_clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debc8fa",
   "metadata": {},
   "source": [
    "## 5. Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02176d0",
   "metadata": {},
   "source": [
    "### Problem (data_loading): Implement data loading (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fc4e3",
   "metadata": {},
   "source": [
    "**Deliverable**: Write a function that takes a numpy array x (integer array with token IDs), a `batch_size`, a `context_length` and a PyTorch device string (e.g., 'cpu' or 'cuda:0'), and returns a pair of tensors: the sampled input sequences and the corresponding next-token targets. Both tensors should have shape (batch_size, context_length) containing token IDs, and both should be placed on the requested device. To test your implementation against our provided tests, you will first need to implement the test adapter at `[adapters.run_get_batch]`. Then, run `uv run pytest -k test_get_batch` to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a438ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_data.py::test_get_batch \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 0.46s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_get_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e3623",
   "metadata": {},
   "source": [
    "### Problem (checkpointing): Implement model checkpointing (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c19794",
   "metadata": {},
   "source": [
    "Implement the following two functions to load and save checkpoints:\n",
    "\n",
    "`def save_checkpoint(model, optimizer, iteration, out)` should dump all the state from the\n",
    "first three parameters into the file-like object out. You can use the state_dict method of both the model and the optimizer to get their relevant states and use `torch.save(obj, out)` to dump obj into out (PyTorch supports either a path or a file-like object here). A typical choice is to have obj be a dictionary, but you can use whatever format you want as long as you can load your checkpoint later.\n",
    "\n",
    "This function expects the following parameters:\n",
    "\n",
    "``` python\n",
    "model: torch.nn.Module\n",
    "optimizer: torch.optim.Optimizer\n",
    "iteration: int\n",
    "out: str | os.PathLike | typing.BinaryIO | typing.IO[bytes]\n",
    "```\n",
    "\n",
    "`def load_checkpoint(src, model, optimizer)` should load a checkpoint from src (path or filelike object), and then recover the model and optimizer states from that checkpoint. Your function should return the iteration number that was saved to the checkpoint. You can use `torch.load(src)` to recover what you saved in your `save_checkpoint` implementation, and the `load_state_dict` method in both the model and optimizers to return them to their previous states.\n",
    "\n",
    "This function expects the following parameters:\n",
    "\n",
    "``` python\n",
    "src: str | os.PathLike | typing.BinaryIO | typing.IO[bytes]\n",
    "model: torch.nn.Module\n",
    "optimizer: torch.optim.Optimizer\n",
    "```\n",
    "\n",
    "Implement the `[adapters.run_save_checkpoint]` and `[adapters.run_load_checkpoint]` adapters, and make sure they pass `uv run pytest -k test_checkpointing`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1679810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.0rc2, pytest-8.3.5, pluggy-1.5.0\n",
      "rootdir: e:\\Code\\CS336\\assignment1-basics\n",
      "configfile: pyproject.toml\n",
      "plugins: jaxtyping-0.3.1\n",
      "collected 48 items / 47 deselected / 1 selected\n",
      "\n",
      "tests/test_serialization.py::test_checkpointing \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m47 deselected\u001b[0m\u001b[32m in 1.62s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv run pytest -k test_checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c31c63e",
   "metadata": {},
   "source": [
    "### Problem (training_together): Put it together (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a3cd5",
   "metadata": {},
   "source": [
    "**Deliverable**: Write a script that runs a training loop to train your model on user-provided input.\n",
    "\n",
    "In particular, we recommend that your training script allow for (at least) the following:\n",
    "\n",
    "- Ability to configure and control the various model and optimizer hyperparameters.\n",
    "- Memory-efficient loading of training and validation large datasets with np.memmap.\n",
    "- Serializing checkpoints to a user-provided path.\n",
    "- Periodically logging training and validation performance (e.g., to console and/or an external service like Weights and Biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba24ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710949",
   "metadata": {},
   "source": [
    "## 6. Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8d09f",
   "metadata": {},
   "source": [
    "Maybe useful: https://zhuanlan.zhihu.com/p/1927311888515073257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c81cb6",
   "metadata": {},
   "source": [
    "### Problem (decoding): Decoding (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8ae08",
   "metadata": {},
   "source": [
    "**Deliverable**: Implement a function to decode from your language model. We recommend that you support the following features:\n",
    "\n",
    "- Generate completions for a user-provided prompt (i.e., take in some x1...t and sample a completion until you hit an `<|endoftext|>` token).\n",
    "- Allow the user to control the maximum number of generated tokens.\n",
    "- Given a desired temperature value, apply softmax temperature scaling to the predicted next-word distributions before sampling.\n",
    "- Top-p sampling ($\\text{Holtzman et al., 2020}$; also referred to as nucleus sampling), given a user-specified threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799684cc",
   "metadata": {},
   "source": [
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dec8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de6c9f6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
